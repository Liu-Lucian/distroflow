"""
Reddit Scraper - Redditçˆ¬è™«
ä½¿ç”¨PRAW (Python Reddit API Wrapper) æˆ–ç›´æ¥APIè®¿é—®
"""

import json
import time
import logging
import requests
from typing import List, Dict, Optional
from src.platform_scraper_base import PlatformScraperBase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RedditScraper(PlatformScraperBase):
    """Redditå¹³å°scraper"""

    def __init__(self, auth_file: str = "platforms_auth.json"):
        """
        åˆå§‹åŒ–Reddit scraper

        Args:
            auth_file: è®¤è¯é…ç½®æ–‡ä»¶è·¯å¾„
        """
        try:
            with open(auth_file, 'r') as f:
                config = json.load(f)
            auth_config = config.get('reddit', {})
        except FileNotFoundError:
            logger.warning(f"âš ï¸  Auth file {auth_file} not found, using public API")
            auth_config = {}

        super().__init__(auth_config, 'Reddit')

        # Reddit APIé…ç½®
        self.client_id = self.auth_config.get('client_id', '')
        self.client_secret = self.auth_config.get('client_secret', '')
        self.user_agent = self.auth_config.get('user_agent', 'MarketingMindAI/1.0')

        # API endpoints
        self.oauth_url = "https://www.reddit.com/api/v1/access_token"
        self.api_base = "https://oauth.reddit.com"
        self.public_api_base = "https://www.reddit.com"

        # è·å–access tokenï¼ˆå¦‚æœæœ‰credentialsï¼‰
        self.access_token = self._get_access_token()

        # è®¾ç½®headers
        if self.access_token:
            self.headers = {
                'Authorization': f'Bearer {self.access_token}',
                'User-Agent': self.user_agent
            }
        else:
            self.headers = {
                'User-Agent': self.user_agent
            }

    def _get_access_token(self) -> str:
        """
        è·å–Reddit OAuth access token

        Returns:
            Access tokenå­—ç¬¦ä¸²
        """
        if not self.client_id or not self.client_secret:
            logger.info("â„¹ï¸  No Reddit credentials, using public API")
            return ''

        try:
            response = requests.post(
                self.oauth_url,
                auth=(self.client_id, self.client_secret),
                data={'grant_type': 'client_credentials'},
                headers={'User-Agent': self.user_agent},
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                token = data.get('access_token', '')
                if token:
                    logger.info("âœ… Reddit access token obtained")
                return token
            else:
                logger.warning(f"âš ï¸  Failed to get access token: {response.status_code}")
                return ''

        except Exception as e:
            logger.warning(f"âš ï¸  Error getting access token: {e}")
            return ''

    def search_users(self, keywords: List[str], limit: int = 100) -> List[Dict]:
        """
        æœç´¢Redditç”¨æˆ·

        ç­–ç•¥ï¼š
        1. æœç´¢ç›¸å…³subreddits
        2. è·å–çƒ­é—¨å¸–å­çš„ä½œè€…
        3. è·å–æ´»è·ƒè¯„è®ºè€…

        Args:
            keywords: æœç´¢å…³é”®è¯
            limit: ç»“æœæ•°é‡

        Returns:
            ç”¨æˆ·åˆ—è¡¨
        """
        logger.info(f"ğŸ” Searching Reddit for active users (limit: {limit})")

        users = []
        seen_usernames = set()

        # æœç´¢ç›¸å…³subreddits
        query = ' '.join(keywords) if keywords else 'startup'

        try:
            # ä½¿ç”¨å…¬å¼€APIæœç´¢
            base_url = self.api_base if self.access_token else self.public_api_base

            # æœç´¢å¸–å­
            search_url = f"{base_url}/search.json"
            params = {
                'q': query,
                'sort': 'relevance',
                'limit': 25,
                't': 'month'  # æœ€è¿‘ä¸€ä¸ªæœˆ
            }

            response = requests.get(
                search_url,
                params=params,
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                posts = data.get('data', {}).get('children', [])

                logger.info(f"   Found {len(posts)} relevant posts")

                # ä»æ¯ä¸ªå¸–å­æå–ä½œè€…å’Œè¯„è®ºè€…
                for post in posts:
                    if len(users) >= limit:
                        break

                    post_data = post.get('data', {})

                    # è·å–å¸–å­ä½œè€…
                    author = post_data.get('author')
                    if author and author not in ['[deleted]', 'AutoModerator'] and author not in seen_usernames:
                        user_data = self._get_user_data(author)
                        if user_data:
                            users.append(user_data)
                            seen_usernames.add(author)

                    # è·å–è¯„è®ºè€…
                    post_id = post_data.get('id')
                    if post_id and len(users) < limit:
                        commenters = self._get_post_commenters(post_id, limit=5)
                        for commenter in commenters:
                            if len(users) >= limit:
                                break
                            if commenter not in seen_usernames:
                                user_data = self._get_user_data(commenter)
                                if user_data:
                                    users.append(user_data)
                                    seen_usernames.add(commenter)

                    time.sleep(1)  # Rate limiting

            else:
                logger.warning(f"   âš ï¸  Search failed: {response.status_code}")

        except Exception as e:
            logger.error(f"âŒ Error searching Reddit: {e}")

        logger.info(f"âœ… Found {len(users)} users on Reddit")
        return users[:limit]

    def _get_post_commenters(self, post_id: str, limit: int = 5) -> List[str]:
        """
        è·å–å¸–å­çš„è¯„è®ºè€…

        Args:
            post_id: å¸–å­ID
            limit: æ•°é‡é™åˆ¶

        Returns:
            ç”¨æˆ·ååˆ—è¡¨
        """
        commenters = []

        try:
            base_url = self.api_base if self.access_token else self.public_api_base
            comments_url = f"{base_url}/comments/{post_id}.json"

            response = requests.get(
                comments_url,
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                # Redditè¿”å›[post_data, comments_data]
                if len(data) > 1:
                    comments = data[1].get('data', {}).get('children', [])

                    for comment in comments[:limit]:
                        comment_data = comment.get('data', {})
                        author = comment_data.get('author')
                        if author and author not in ['[deleted]', 'AutoModerator']:
                            commenters.append(author)

        except Exception as e:
            logger.debug(f"   Error getting commenters: {e}")

        return commenters

    def _get_user_data(self, username: str) -> Optional[Dict]:
        """
        è·å–ç”¨æˆ·æ•°æ®

        Args:
            username: Redditç”¨æˆ·å

        Returns:
            ç”¨æˆ·æ•°æ®å­—å…¸
        """
        try:
            base_url = self.api_base if self.access_token else self.public_api_base
            user_url = f"{base_url}/user/{username}/about.json"

            response = requests.get(
                user_url,
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                user_data = data.get('data', {})

                # è¿‡æ»¤ä½karmaç”¨æˆ·
                link_karma = user_data.get('link_karma', 0)
                comment_karma = user_data.get('comment_karma', 0)
                total_karma = link_karma + comment_karma

                if total_karma < 100:  # åªè¦karma>100çš„ç”¨æˆ·
                    return None

                user = {
                    'username': username,
                    'profile_url': f"https://www.reddit.com/user/{username}",
                    'karma': total_karma,
                    'link_karma': link_karma,
                    'comment_karma': comment_karma,
                    'created_utc': user_data.get('created_utc', 0),
                    'platform': 'reddit'
                }

                return user

        except Exception as e:
            logger.debug(f"   Error getting user {username}: {e}")
            return None

    def get_user_profile(self, user_id: str) -> Dict:
        """
        è·å–Redditç”¨æˆ·è¯¦ç»†èµ„æ–™

        Args:
            user_id: ç”¨æˆ·å

        Returns:
            ç”¨æˆ·è¯¦ç»†èµ„æ–™
        """
        logger.debug(f"ğŸ“– Fetching Reddit profile: {user_id}")

        user_data = self._get_user_data(user_id)

        if user_data:
            return user_data
        else:
            return {
                'username': user_id,
                'profile_url': f"https://www.reddit.com/user/{user_id}",
                'platform': 'reddit',
                'status': 'not_found'
            }

    def extract_email(self, user_profile: Dict) -> Optional[str]:
        """
        ä»Redditèµ„æ–™æå–é‚®ç®±

        Redditä¸å…¬å¼€é‚®ç®±ï¼Œéœ€è¦ä»ç”¨æˆ·çš„å¸–å­å’Œè¯„è®ºä¸­æœç´¢

        Args:
            user_profile: ç”¨æˆ·èµ„æ–™

        Returns:
            é‚®ç®±åœ°å€æˆ–None
        """
        # Redditä¸å…¬å¼€é‚®ç®±
        # å¯ä»¥å°è¯•ä»ç”¨æˆ·çš„å¸–å­å†å²ä¸­æœç´¢ï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†
        return None

    def search_subreddit_users(self, subreddit: str, limit: int = 50) -> List[Dict]:
        """
        ä»ç‰¹å®šsubredditè·å–æ´»è·ƒç”¨æˆ·

        Args:
            subreddit: subredditåç§°ï¼ˆä¾‹å¦‚'startups', 'entrepreneur'ï¼‰
            limit: æ•°é‡é™åˆ¶

        Returns:
            ç”¨æˆ·åˆ—è¡¨
        """
        logger.info(f"ğŸ” Searching r/{subreddit} for active users...")

        users = []
        seen_usernames = set()

        try:
            base_url = self.api_base if self.access_token else self.public_api_base
            subreddit_url = f"{base_url}/r/{subreddit}/hot.json"

            response = requests.get(
                subreddit_url,
                params={'limit': 25},
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                posts = data.get('data', {}).get('children', [])

                for post in posts:
                    if len(users) >= limit:
                        break

                    post_data = post.get('data', {})
                    author = post_data.get('author')

                    if author and author not in ['[deleted]', 'AutoModerator'] and author not in seen_usernames:
                        user_data = self._get_user_data(author)
                        if user_data:
                            users.append(user_data)
                            seen_usernames.add(author)

                    time.sleep(0.5)

        except Exception as e:
            logger.error(f"âŒ Error: {e}")

        logger.info(f"âœ… Found {len(users)} users from r/{subreddit}")
        return users


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    scraper = RedditScraper()

    # æµ‹è¯•æœç´¢ç”¨æˆ·
    users = scraper.search_users(["startup", "founder"], limit=10)

    print(f"\nâœ… Found {len(users)} users:")
    for user in users:
        print(f"  - u/{user.get('username')} (karma: {user.get('karma', 0)})")
        print(f"    Profile: {user.get('profile_url')}")

    # æµ‹è¯•subredditæœç´¢
    print("\nğŸ” Searching r/startups...")
    startup_users = scraper.search_subreddit_users('startups', limit=5)
    print(f"âœ… Found {len(startup_users)} users from r/startups")
