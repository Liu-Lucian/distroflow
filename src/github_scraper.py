"""
GitHub Scraper - GitHubç”¨æˆ·çˆ¬è™«
ä½¿ç”¨GitHub REST API v3
"""

import json
import logging
import requests
from typing import List, Dict, Optional
from src.platform_scraper_base import PlatformScraperBase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GitHubScraper(PlatformScraperBase):
    """GitHubå¹³å°scraper"""

    def __init__(self, auth_file: str = "platforms_auth.json"):
        """
        åˆå§‹åŒ–GitHub scraper

        Args:
            auth_file: è®¤è¯é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½è®¤è¯é…ç½®
        with open(auth_file, 'r') as f:
            config = json.load(f)

        super().__init__(config['github'], 'GitHub')

        self.access_token = self.auth_config['access_token']
        self.headers = {
            'Authorization': f'token {self.access_token}',
            'Accept': 'application/vnd.github+json',
            'X-GitHub-Api-Version': '2022-11-28'
        }
        self.base_url = 'https://api.github.com'

    def search_users(self, keywords: List[str], limit: int = 100) -> List[Dict]:
        """
        æœç´¢GitHubç”¨æˆ·

        Args:
            keywords: æœç´¢å…³é”®è¯
            limit: ç»“æœæ•°é‡

        Returns:
            ç”¨æˆ·åˆ—è¡¨
        """
        # æ„å»ºæœç´¢æŸ¥è¯¢
        query = ' '.join(keywords)

        logger.info(f"ğŸ” Searching GitHub: {query}")

        users = []
        page = 1
        per_page = min(30, limit)  # GitHub APIæ¯é¡µæœ€å¤š100ä¸ªï¼Œæˆ‘ä»¬ç”¨30

        while len(users) < limit:
            try:
                # GitHub User Search API
                url = f"{self.base_url}/search/users"
                params = {
                    'q': query,
                    'per_page': per_page,
                    'page': page
                }

                response = requests.get(url, headers=self.headers, params=params)
                response.raise_for_status()

                data = response.json()
                items = data.get('items', [])

                if not items:
                    break  # æ²¡æœ‰æ›´å¤šç»“æœ

                for item in items:
                    if len(users) >= limit:
                        break

                    user = {
                        'username': item['login'],
                        'id': item['login'],  # ä½¿ç”¨usernameä½œä¸ºid
                        'profile_url': item['html_url'],
                        'avatar_url': item.get('avatar_url', ''),
                        'platform': 'github'
                    }
                    users.append(user)

                page += 1

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šç»“æœ
                if len(items) < per_page:
                    break

            except requests.exceptions.RequestException as e:
                logger.error(f"âŒ GitHub API error: {e}")
                break

        logger.info(f"âœ… Found {len(users)} users on GitHub")
        return users[:limit]

    def get_user_profile(self, user_id: str) -> Dict:
        """
        è·å–GitHubç”¨æˆ·è¯¦ç»†èµ„æ–™

        Args:
            user_id: ç”¨æˆ·å

        Returns:
            ç”¨æˆ·è¯¦ç»†èµ„æ–™
        """
        username = user_id
        logger.debug(f"ğŸ“– Fetching profile: {username}")

        try:
            # Get user details
            url = f"{self.base_url}/users/{username}"
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()

            data = response.json()

            profile = {
                'username': data['login'],
                'name': data.get('name', ''),
                'bio': data.get('bio', ''),
                'location': data.get('location', ''),
                'website': data.get('blog', ''),
                'email': data.get('email', ''),  # å…¬å¼€é‚®ç®±ï¼ˆå¦‚æœæœ‰ï¼‰
                'company': data.get('company', ''),
                'twitter_username': data.get('twitter_username', ''),
                'followers_count': data.get('followers', 0),
                'following_count': data.get('following', 0),
                'public_repos': data.get('public_repos', 0),
                'profile_url': data['html_url'],
                'avatar_url': data.get('avatar_url', ''),
                'platform': 'github'
            }

            # å¦‚æœprofileæ²¡æœ‰é‚®ç®±ï¼Œå°è¯•ä»å¤šä¸ªæ¥æºè·å–
            if not profile.get('email'):
                # ä¼˜å…ˆçº§1: READMEæ–‡ä»¶ï¼ˆæœ€å¸¸è§ï¼‰
                email_from_readme = self._get_email_from_readme(username)
                if email_from_readme:
                    profile['email'] = email_from_readme
                else:
                    # ä¼˜å…ˆçº§2: commits
                    email_from_events = self._get_email_from_events(username)
                    if email_from_events:
                        profile['email'] = email_from_events

            return profile

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error fetching GitHub profile {username}: {e}")
            return {
                'username': username,
                'platform': 'github',
                'profile_url': f'https://github.com/{username}'
            }

    def _get_email_from_readme(self, username: str) -> Optional[str]:
        """
        ä»ç”¨æˆ·çš„profile READMEä¸­æå–é‚®ç®±

        Args:
            username: ç”¨æˆ·å

        Returns:
            é‚®ç®±åœ°å€æˆ–None
        """
        import re

        try:
            # GitHub profile READMEä½äº username/username ä»“åº“
            url = f"{self.base_url}/repos/{username}/{username}/readme"
            response = requests.get(url, headers=self.headers)

            if response.status_code == 200:
                readme_data = response.json()
                # README contentæ˜¯base64ç¼–ç çš„
                import base64
                content = base64.b64decode(readme_data['content']).decode('utf-8', errors='ignore')

                # æ­£åˆ™æå–é‚®ç®±
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails = re.findall(email_pattern, content)

                # è¿‡æ»¤noreplyå’Œexampleé‚®ç®±
                for email in emails:
                    email_lower = email.lower()
                    if 'noreply' not in email_lower and 'example' not in email_lower and 'your' not in email_lower:
                        logger.debug(f"   Found email from README: {email}")
                        return email

        except Exception as e:
            logger.debug(f"   Could not extract email from README: {e}")

        return None

    def _get_email_from_events(self, username: str) -> Optional[str]:
        """
        ä»ç”¨æˆ·çš„å…¬å¼€eventsä¸­æå–é‚®ç®±ï¼ˆä»commitä¸­ï¼‰

        Args:
            username: ç”¨æˆ·å

        Returns:
            é‚®ç®±åœ°å€æˆ–None
        """
        try:
            url = f"{self.base_url}/users/{username}/events/public"
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()

            events = response.json()

            # æŸ¥æ‰¾PushEventä¸­çš„commité‚®ç®±
            for event in events[:10]:  # åªæ£€æŸ¥æœ€è¿‘10ä¸ªevents
                if event.get('type') == 'PushEvent':
                    commits = event.get('payload', {}).get('commits', [])
                    for commit in commits:
                        author = commit.get('author', {})
                        email = author.get('email', '')

                        # è¿‡æ»¤noreplyé‚®ç®±
                        if email and 'noreply' not in email.lower():
                            logger.debug(f"   Found email from commits: {email}")
                            return email

        except Exception as e:
            logger.debug(f"   Could not extract email from events: {e}")

        return None

    def extract_email(self, user_profile: Dict) -> Optional[str]:
        """
        ä»GitHubèµ„æ–™æå–é‚®ç®±

        Args:
            user_profile: ç”¨æˆ·èµ„æ–™

        Returns:
            é‚®ç®±åœ°å€æˆ–None
        """
        # GitHubå¯èƒ½å…¬å¼€é‚®ç®±
        email = user_profile.get('email', '')

        if email and 'noreply' not in email.lower():
            return email

        # å¦‚æœæ²¡æœ‰ï¼Œè¿”å›Noneè®©Hunter.ioå¤„ç†
        return None

    def search_by_repository(self, repo_name: str, limit: int = 50) -> List[Dict]:
        """
        è·å–æŸä¸ªä»“åº“çš„contributors

        Args:
            repo_name: ä»“åº“åï¼ˆæ ¼å¼: owner/repoï¼‰
            limit: æ•°é‡é™åˆ¶

        Returns:
            ç”¨æˆ·åˆ—è¡¨
        """
        logger.info(f"ğŸ” Searching contributors of {repo_name}")

        try:
            url = f"{self.base_url}/repos/{repo_name}/contributors"
            params = {'per_page': min(100, limit)}

            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()

            contributors = response.json()

            users = []
            for contributor in contributors[:limit]:
                user = {
                    'username': contributor['login'],
                    'id': contributor['login'],
                    'profile_url': contributor['html_url'],
                    'avatar_url': contributor.get('avatar_url', ''),
                    'contributions': contributor.get('contributions', 0),
                    'platform': 'github'
                }
                users.append(user)

            logger.info(f"âœ… Found {len(users)} contributors")
            return users

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error fetching contributors: {e}")
            return []

    def search_by_topic(self, topic: str, limit: int = 100) -> List[Dict]:
        """
        é€šè¿‡topicæœç´¢æ´»è·ƒç”¨æˆ·

        Args:
            topic: ä¸»é¢˜ï¼ˆå¦‚: machine-learning, ai, interviewï¼‰
            limit: æ•°é‡é™åˆ¶

        Returns:
            ç”¨æˆ·åˆ—è¡¨
        """
        logger.info(f"ğŸ” Searching GitHub users by topic: {topic}")

        try:
            # å…ˆæœç´¢è¯¥topicçš„çƒ­é—¨ä»“åº“
            url = f"{self.base_url}/search/repositories"
            params = {
                'q': f'topic:{topic}',
                'sort': 'stars',
                'per_page': 10
            }

            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()

            repos = response.json().get('items', [])

            # ä»è¿™äº›ä»“åº“æ”¶é›†contributors
            all_users = []
            seen_usernames = set()

            for repo in repos:
                if len(all_users) >= limit:
                    break

                repo_full_name = repo['full_name']
                contributors = self.search_by_repository(
                    repo_full_name,
                    limit=min(20, limit - len(all_users))
                )

                # å»é‡
                for user in contributors:
                    if user['username'] not in seen_usernames:
                        all_users.append(user)
                        seen_usernames.add(user['username'])

                    if len(all_users) >= limit:
                        break

            logger.info(f"âœ… Found {len(all_users)} users for topic {topic}")
            return all_users[:limit]

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error searching by topic: {e}")
            return []


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    scraper = GitHubScraper()

    # æµ‹è¯•æœç´¢
    test_keywords = ["recruiter", "hiring", "interview"]
    users = scraper.search_users(test_keywords, limit=5)

    print(f"\nâœ… Found {len(users)} users:")
    for user in users:
        print(f"  - {user['username']} | {user.get('profile_url', 'N/A')}")

    # æµ‹è¯•è·å–è¯¦æƒ…
    if users:
        profile = scraper.get_user_profile(users[0]['username'])
        print(f"\nğŸ“– Profile details:")
        print(f"  Name: {profile.get('name')}")
        print(f"  Email: {profile.get('email', 'Not public')}")
        print(f"  Company: {profile.get('company')}")
        print(f"  Location: {profile.get('location')}")
        print(f"  Followers: {profile.get('followers_count')}")

    # æµ‹è¯•topicæœç´¢
    print("\nğŸ” Testing topic search...")
    topic_users = scraper.search_by_topic("interview", limit=5)
    print(f"âœ… Found {len(topic_users)} users for topic 'interview'")
