"""
Smart User Finder - AIé©±åŠ¨çš„æ½œåœ¨å®¢æˆ·è¯†åˆ«ç³»ç»Ÿ
åˆ†æå¸–å­è¯„è®ºï¼Œä½¿ç”¨GPTè¯†åˆ«æœ‰éœ€æ±‚çš„ç”¨æˆ·
"""

import os
import json
import logging
from typing import List, Dict, Optional
from openai import OpenAI
from playwright.sync_api import Page, sync_playwright

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SmartUserFinder:
    """AIé©±åŠ¨çš„ç”¨æˆ·è¯†åˆ«å™¨ - ä»è¯„è®ºä¸­æ‰¾åˆ°æ½œåœ¨å®¢æˆ·"""

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–Smart User Finder

        Args:
            api_key: OpenAI API key
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key not found")

        # Strip whitespace and newlines from API key
        self.api_key = self.api_key.strip()

        self.client = OpenAI(api_key=self.api_key)
        logger.info("âœ… Smart User Finder initialized")

    def analyze_comments_for_intent(
        self,
        comments: List[Dict],
        product_description: str
    ) -> List[Dict]:
        """
        ä½¿ç”¨AIåˆ†æè¯„è®ºï¼Œè¯†åˆ«æœ‰è´­ä¹°æ„å›¾çš„ç”¨æˆ·

        Args:
            comments: è¯„è®ºåˆ—è¡¨ [{"username": "...", "text": "...", "url": "..."}, ...]
            product_description: äº§å“æè¿°

        Returns:
            æœ‰æ„å›¾çš„ç”¨æˆ·åˆ—è¡¨ï¼Œå¸¦æœ‰æ„å›¾åˆ†æ•°å’ŒåŸå› 
        """
        if not comments:
            return []

        logger.info(f"ğŸ§  Analyzing {len(comments)} comments with AI...")

        # æ„å»ºprompt
        comments_text = "\n".join([
            f"- @{c['username']}: {c['text']}"
            for c in comments[:50]  # é™åˆ¶åœ¨50æ¡ä»¥å†…é¿å…tokenè¿‡å¤š
        ])

        prompt = f"""You are a sales intelligence AI. Analyze these social media comments and identify users who might be interested in our product.

**Our Product**: {product_description}

**Comments to Analyze**:
{comments_text}

**Task**: For each comment, determine if the user shows:
1. Pain points our product solves
2. Interest in the topic
3. Buying intent or need for solution
4. Decision-making authority (e.g., founder, CEO, hiring manager)

**Output Format** (JSON array):
[
    {{
        "username": "username without @",
        "intent_score": 0.0-1.0,
        "reasons": ["reason 1", "reason 2"],
        "pain_points": ["pain point 1", "pain point 2"],
        "priority": "high" | "medium" | "low"
    }},
    ...
]

Only include users with intent_score >= 0.4. Return valid JSON only.
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # æ›´ä¾¿å®œçš„æ¨¡å‹ç”¨äºæ‰¹é‡åˆ†æ
                messages=[
                    {"role": "system", "content": "You are a sales intelligence analyst. Output valid JSON only."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3
            )

            response_text = response.choices[0].message.content

            # æå–JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()

            qualified_users = json.loads(response_text)

            logger.info(f"âœ… AI identified {len(qualified_users)} qualified users")

            # åˆå¹¶åŸå§‹è¯„è®ºä¿¡æ¯
            username_to_comment = {c['username'].lstrip('@'): c for c in comments}

            for user in qualified_users:
                username = user['username'].lstrip('@')
                if username in username_to_comment:
                    user.update(username_to_comment[username])

            return qualified_users

        except Exception as e:
            logger.error(f"âŒ AI analysis failed: {e}")
            import traceback
            traceback.print_exc()
            return []

    def scrape_post_comments(
        self,
        page: Page,
        post_url: str,
        platform: str = "reddit",
        max_comments: int = 50
    ) -> List[Dict]:
        """
        ä»å¸–å­ä¸­æŠ“å–è¯„è®º

        Args:
            page: Playwright pageå¯¹è±¡
            post_url: å¸–å­URL
            platform: å¹³å°åç§° (reddit, twitter, instagram)
            max_comments: æœ€å¤šæŠ“å–è¯„è®ºæ•°

        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        logger.info(f"ğŸ“ Scraping comments from {post_url}...")

        page.goto(post_url, timeout=60000)

        import time
        time.sleep(3)

        comments = []

        if platform == "reddit":
            # Redditè¯„è®ºé€‰æ‹©å™¨
            comment_selectors = [
                'div[data-testid="comment"]',
                'div.Comment',
                'div[id^="t1_"]'
            ]

            for selector in comment_selectors:
                try:
                    comment_elements = page.query_selector_all(selector)
                    if comment_elements:
                        logger.info(f"   Found {len(comment_elements)} comments with: {selector}")

                        for elem in comment_elements[:max_comments]:
                            try:
                                # ç”¨æˆ·å
                                username_elem = elem.query_selector('a[href*="/user/"]')
                                username = username_elem.inner_text() if username_elem else "unknown"

                                # è¯„è®ºæ–‡æœ¬
                                text_elem = elem.query_selector('div[data-testid="comment"], p, div')
                                text = text_elem.inner_text() if text_elem else ""

                                if username and text and len(text) > 10:
                                    comments.append({
                                        "username": username.lstrip('u/').lstrip('@'),
                                        "text": text[:500],  # é™åˆ¶é•¿åº¦
                                        "platform": platform,
                                        "source_url": post_url
                                    })

                            except Exception as e:
                                logger.debug(f"Error parsing comment: {e}")
                                continue

                        if comments:
                            break
                except Exception as e:
                    logger.debug(f"Selector {selector} failed: {e}")
                    continue

        elif platform == "twitter":
            # Twitterè¯„è®ºé€‰æ‹©å™¨
            tweet_selectors = [
                'article[data-testid="tweet"]',
                'div[data-testid="cellInnerDiv"]'
            ]

            # æ»šåŠ¨åŠ è½½è¯„è®º
            for i in range(3):
                page.evaluate("window.scrollBy(0, 1000)")
                time.sleep(1)

            for selector in tweet_selectors:
                try:
                    tweet_elements = page.query_selector_all(selector)
                    if tweet_elements:
                        logger.info(f"   Found {len(tweet_elements)} tweets/replies")

                        for elem in tweet_elements[:max_comments]:
                            try:
                                # ç”¨æˆ·å
                                username_elem = elem.query_selector('a[href^="/"][href*="/status/"] span')
                                username = username_elem.inner_text() if username_elem else "unknown"

                                # æ¨æ–‡æ–‡æœ¬
                                text_elem = elem.query_selector('div[data-testid="tweetText"], div[lang]')
                                text = text_elem.inner_text() if text_elem else ""

                                if username and text and len(text) > 10:
                                    comments.append({
                                        "username": username.lstrip('@'),
                                        "text": text[:500],
                                        "platform": platform,
                                        "source_url": post_url
                                    })

                            except Exception as e:
                                logger.debug(f"Error parsing tweet: {e}")
                                continue

                        if comments:
                            break
                except Exception as e:
                    logger.debug(f"Selector {selector} failed: {e}")
                    continue

        elif platform == "instagram":
            # Instagramè¯„è®ºé€‰æ‹©å™¨
            # å…ˆåŠ è½½æ›´å¤šè¯„è®º
            try:
                load_more = page.query_selector('button:has-text("Load more comments")')
                if load_more:
                    load_more.click()
                    time.sleep(2)
            except:
                pass

            comment_selectors = [
                'ul li div span',
                'div[role="button"] span',
            ]

            # Instagramè¯„è®ºç»“æ„æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            # ç®€åŒ–ç‰ˆï¼šæŠ“å–å¯è§çš„è¯„è®ºæ–‡æœ¬
            try:
                all_spans = page.query_selector_all('span')
                for span in all_spans[:max_comments]:
                    text = span.inner_text()
                    # ç®€å•å¯å‘å¼ï¼šè¯„è®ºé€šå¸¸é•¿åº¦åœ¨10-500å­—ç¬¦
                    if 10 < len(text) < 500 and not text.startswith('http'):
                        comments.append({
                            "username": "instagram_user",  # Instagramè¯„è®ºç”¨æˆ·åè¾ƒéš¾æå–
                            "text": text,
                            "platform": platform,
                            "source_url": post_url
                        })
            except Exception as e:
                logger.error(f"Instagram comment scraping failed: {e}")

        # å»é‡
        seen = set()
        unique_comments = []
        for c in comments:
            key = f"{c['username']}:{c['text'][:50]}"
            if key not in seen:
                seen.add(key)
                unique_comments.append(c)

        logger.info(f"âœ… Scraped {len(unique_comments)} unique comments")

        return unique_comments

    def find_qualified_users_from_post(
        self,
        page: Page,
        post_url: str,
        product_description: str,
        platform: str = "reddit",
        max_comments: int = 50
    ) -> List[Dict]:
        """
        å®Œæ•´æµç¨‹ï¼šæŠ“å–è¯„è®º â†’ AIåˆ†æ â†’ è¿”å›åˆæ ¼ç”¨æˆ·

        Args:
            page: Playwright pageå¯¹è±¡
            post_url: å¸–å­URL
            product_description: äº§å“æè¿°
            platform: å¹³å°åç§°
            max_comments: æœ€å¤šæŠ“å–è¯„è®ºæ•°

        Returns:
            åˆæ ¼ç”¨æˆ·åˆ—è¡¨
        """
        # æ­¥éª¤1: æŠ“å–è¯„è®º
        comments = self.scrape_post_comments(
            page=page,
            post_url=post_url,
            platform=platform,
            max_comments=max_comments
        )

        if not comments:
            logger.warning("âš ï¸ No comments found")
            return []

        # æ­¥éª¤2: AIåˆ†æ
        qualified_users = self.analyze_comments_for_intent(
            comments=comments,
            product_description=product_description
        )

        return qualified_users


# ä¾¿æ·å‡½æ•°
def find_users_in_post(
    post_url: str,
    product_description: str,
    platform: str = "reddit"
) -> List[Dict]:
    """
    å¿«é€Ÿä»å•ä¸ªå¸–å­ä¸­æ‰¾åˆ°æ½œåœ¨å®¢æˆ·

    Args:
        post_url: å¸–å­URL
        product_description: äº§å“æè¿°
        platform: å¹³å°åç§°

    Returns:
        åˆæ ¼ç”¨æˆ·åˆ—è¡¨
    """
    finder = SmartUserFinder()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()

        users = finder.find_qualified_users_from_post(
            page=page,
            post_url=post_url,
            product_description=product_description,
            platform=platform
        )

        browser.close()

    return users


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("Smart User Finder - AI-powered lead generation")
    print("\nExample usage:")
    print("""
from smart_user_finder import SmartUserFinder

finder = SmartUserFinder()

# æ‰¾åˆ°æ½œåœ¨å®¢æˆ·
users = finder.find_qualified_users_from_post(
    page=page,
    post_url="https://reddit.com/r/jobs/comments/...",
    product_description="HireMeAI - AI-powered interview preparation platform",
    platform="reddit"
)

# ç»“æœ
for user in users:
    print(f"@{user['username']} - Score: {user['intent_score']}")
    print(f"  Reasons: {user['reasons']}")
    print(f"  Priority: {user['priority']}")
""")
