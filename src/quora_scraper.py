#!/usr/bin/env python3
"""
Quoraé—®é¢˜æœç´¢å™¨ - Simple Pattern
æ ¸å¿ƒåŠŸèƒ½ï¼šæ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³é—®é¢˜
"""

from playwright.sync_api import sync_playwright, Page
import time
import logging
import random
from typing import List, Dict, Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QuoraScraper:
    """Quoraé—®é¢˜æœç´¢å™¨ - ç®€å•æ¨¡å¼"""

    def __init__(self, auth_config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–Quora scraper

        Args:
            auth_config: è®¤è¯é…ç½® (cookies)
        """
        self.auth_config = auth_config or {}
        self.playwright = None
        self.browser = None
        self.page = None

    def setup_browser(self, headless: bool = True):
        """è®¾ç½®æµè§ˆå™¨"""
        logger.info("ğŸŒ å¯åŠ¨æµè§ˆå™¨...")
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(headless=headless)

        # åˆ›å»ºä¸Šä¸‹æ–‡å¹¶æ·»åŠ cookies
        context = self.browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        )

        # æ·»åŠ cookiesï¼ˆå¦‚æœæœ‰ï¼‰
        if self.auth_config.get('cookies'):
            context.add_cookies(self.auth_config['cookies'])
            logger.info("   âœ… Cookieså·²åŠ è½½")

        self.page = context.new_page()
        logger.info("   âœ… æµè§ˆå™¨å·²å¯åŠ¨")

    def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        logger.info("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")

    def _random_delay(self, min_sec: float = 1, max_sec: float = 3):
        """éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        delay = random.uniform(min_sec, max_sec)
        time.sleep(delay)

    def search_questions(self, keywords: str, max_questions: int = 10) -> List[Dict]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢Quoraé—®é¢˜ - æ ¸å¿ƒå‡½æ•°

        Args:
            keywords: æœç´¢å…³é”®è¯
            max_questions: æœ€å¤šè¿”å›å¤šå°‘ä¸ªé—®é¢˜

        Returns:
            é—®é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å«:
            - question_text: é—®é¢˜æ ‡é¢˜
            - question_url: é—®é¢˜é“¾æ¥
            - answer_count: å›ç­”æ•°é‡
            - follower_count: å…³æ³¨æ•°é‡
        """
        logger.info(f"ğŸ” æœç´¢Quoraé—®é¢˜: '{keywords}'")
        logger.info(f"   ç›®æ ‡: {max_questions} ä¸ªé—®é¢˜")

        if not self.page:
            raise Exception("æµè§ˆå™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ setup_browser()")

        questions = []

        try:
            # è®¿é—®Quoraæœç´¢é¡µé¢
            search_url = f"https://www.quora.com/search?q={keywords.replace(' ', '+')}"
            logger.info(f"   è®¿é—®æœç´¢é¡µé¢...")
            self.page.goto(search_url, wait_until="domcontentloaded", timeout=30000)
            self._random_delay(2, 4)

            # ç­‰å¾…å†…å®¹åŠ è½½
            try:
                self.page.wait_for_selector('div[class*="Question"]', timeout=10000)
            except:
                logger.warning("   âš ï¸  æœç´¢ç»“æœåŠ è½½è¾ƒæ…¢ï¼Œç»§ç»­å°è¯•...")

            # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
            for i in range(3):
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                self._random_delay(1, 2)

            # æå–é—®é¢˜ - å°è¯•å¤šä¸ªé€‰æ‹©å™¨
            question_selectors = [
                'a[class*="question_link"]',
                'a[href*="/question/"]',
                'div[class*="Question"] a',
                'span[class*="QuestionText"] a',
            ]

            question_elements = []
            for selector in question_selectors:
                elements = self.page.query_selector_all(selector)
                if elements:
                    logger.info(f"   âœ… ä½¿ç”¨é€‰æ‹©å™¨æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ : {selector}")
                    question_elements = elements
                    break

            if not question_elements:
                logger.error("   âŒ æœªæ‰¾åˆ°é—®é¢˜å…ƒç´ ")
                return []

            # æå–é—®é¢˜ä¿¡æ¯
            seen_urls = set()
            for elem in question_elements:
                if len(questions) >= max_questions:
                    break

                try:
                    # è·å–é—®é¢˜æ–‡æœ¬å’Œé“¾æ¥
                    question_text = elem.inner_text().strip()
                    question_href = elem.get_attribute('href')

                    if not question_text or not question_href:
                        continue

                    # æ„é€ å®Œæ•´URL
                    if question_href.startswith('/'):
                        question_url = f"https://www.quora.com{question_href}"
                    else:
                        question_url = question_href

                    # å»é‡
                    if question_url in seen_urls:
                        continue
                    seen_urls.add(question_url)

                    # è¿‡æ»¤éé—®é¢˜é“¾æ¥
                    if '/question/' not in question_url:
                        continue

                    question_data = {
                        'question_text': question_text,
                        'question_url': question_url,
                        'answer_count': 0,  # éœ€è¦è®¿é—®è¯¦æƒ…é¡µæ‰èƒ½è·å–
                        'follower_count': 0,
                        'keywords': keywords,
                        'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
                    }

                    questions.append(question_data)
                    logger.info(f"   ğŸ“„ æ‰¾åˆ°é—®é¢˜ {len(questions)}: {question_text[:60]}...")

                except Exception as e:
                    logger.debug(f"      è·³è¿‡ä¸€ä¸ªå…ƒç´ : {str(e)[:50]}")
                    continue

            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(questions)} ä¸ªé—®é¢˜")
            return questions

        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def get_question_details(self, question_url: str) -> Dict:
        """
        è·å–é—®é¢˜è¯¦æƒ…ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

        Args:
            question_url: é—®é¢˜URL

        Returns:
            é—®é¢˜è¯¦ç»†ä¿¡æ¯
        """
        logger.info(f"ğŸ“– è·å–é—®é¢˜è¯¦æƒ…: {question_url}")

        try:
            self.page.goto(question_url, wait_until="domcontentloaded", timeout=30000)
            self._random_delay(2, 3)

            # è·å–é—®é¢˜æ ‡é¢˜
            question_text = ""
            try:
                title_elem = self.page.query_selector('h1, span[class*="QuestionText"]')
                if title_elem:
                    question_text = title_elem.inner_text().strip()
            except:
                pass

            # è·å–å›ç­”æ•°é‡
            answer_count = 0
            try:
                answer_elems = self.page.query_selector_all('div[class*="Answer"]')
                answer_count = len(answer_elems)
            except:
                pass

            return {
                'question_url': question_url,
                'question_text': question_text,
                'answer_count': answer_count,
                'retrieved_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }

        except Exception as e:
            logger.error(f"âŒ è·å–è¯¦æƒ…å¤±è´¥: {str(e)}")
            return {}


if __name__ == "__main__":
    """æµ‹è¯•è„šæœ¬"""
    import json

    # æµ‹è¯•æœç´¢
    scraper = QuoraScraper()

    try:
        scraper.setup_browser(headless=False)

        # æµ‹è¯•æœç´¢é—®é¢˜
        questions = scraper.search_questions("AI interview assistant", max_questions=5)

        print(f"\næ‰¾åˆ° {len(questions)} ä¸ªé—®é¢˜:")
        print(json.dumps(questions, indent=2, ensure_ascii=False))

        # ä¿å­˜ç»“æœ
        with open('quora_test_questions.json', 'w', encoding='utf-8') as f:
            json.dump(questions, f, indent=2, ensure_ascii=False)
        print("\nâœ… ç»“æœå·²ä¿å­˜åˆ° quora_test_questions.json")

    finally:
        input("\næŒ‰Enterå…³é—­æµè§ˆå™¨...")
        scraper.close_browser()
