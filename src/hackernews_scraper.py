"""
Hacker News Scraper - Hacker Newsçˆ¬è™«
é€šè¿‡Hacker News APIå’ŒAlgolia APIè·å–æŠ€æœ¯äººå‘˜å’Œåˆ›å§‹äººçš„ä¿¡æ¯
"""

import json
import time
import logging
import requests
from typing import List, Dict, Optional
from src.platform_scraper_base import PlatformScraperBase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HackerNewsScraper(PlatformScraperBase):
    """Hacker Newså¹³å°scraper"""

    def __init__(self, auth_file: str = "platforms_auth.json"):
        """
        åˆå§‹åŒ–Hacker News scraper

        Args:
            auth_file: è®¤è¯é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆHNä¸éœ€è¦è®¤è¯ï¼‰
        """
        super().__init__({}, 'Hacker News')

        # Hacker News API (å®˜æ–¹)
        self.api_base = "https://hacker-news.firebaseio.com/v0"

        # Algolia HN Search API (æ›´å¼ºå¤§)
        self.algolia_base = "https://hn.algolia.com/api/v1"

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

    def search_users(self, keywords: List[str], limit: int = 100) -> List[Dict]:
        """
        æœç´¢Hacker Newsç”¨æˆ·

        ç­–ç•¥ï¼š
        1. æœç´¢ç›¸å…³çš„Show HN/Ask HNå¸–å­
        2. è·å–å‘å¸–è€…å’Œæ´»è·ƒè¯„è®ºè€…
        3. ä¼˜å…ˆé€‰æ‹©karmaé«˜çš„ç”¨æˆ·

        Args:
            keywords: æœç´¢å…³é”®è¯
            limit: ç»“æœæ•°é‡

        Returns:
            ç”¨æˆ·åˆ—è¡¨
        """
        logger.info(f"ğŸ” Searching Hacker News for active users (limit: {limit})")

        users = []
        seen_usernames = set()

        # æœç´¢å…³é”®è¯ç»„åˆ
        query = ' '.join(keywords) if keywords else 'hiring'

        try:
            # ä½¿ç”¨Algolia APIæœç´¢ç›¸å…³å¸–å­
            search_url = f"{self.algolia_base}/search"
            params = {
                'query': query,
                'tags': '(story,show_hn)',  # åªæœç´¢æ•…äº‹å’ŒShow HN
                'hitsPerPage': 30
            }

            response = requests.get(
                search_url,
                params=params,
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                hits = data.get('hits', [])

                logger.info(f"   Found {len(hits)} relevant posts")

                # ä»æ¯ä¸ªå¸–å­æå–ä½œè€…å’Œè¯„è®ºè€…
                for hit in hits:
                    if len(users) >= limit:
                        break

                    # è·å–å¸–å­ä½œè€…
                    author = hit.get('author')
                    if author and author not in seen_usernames:
                        user_data = self._get_user_data(author)
                        if user_data:
                            users.append(user_data)
                            seen_usernames.add(author)

                    # è·å–å¸–å­çš„è¯„è®ºè€…
                    story_id = hit.get('objectID')
                    if story_id:
                        commenters = self._get_story_commenters(story_id, limit=5)
                        for commenter in commenters:
                            if len(users) >= limit:
                                break
                            if commenter not in seen_usernames:
                                user_data = self._get_user_data(commenter)
                                if user_data:
                                    users.append(user_data)
                                    seen_usernames.add(commenter)

                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

            else:
                logger.warning(f"   âš ï¸  Search failed: {response.status_code}")

        except Exception as e:
            logger.error(f"âŒ Error searching Hacker News: {e}")

        logger.info(f"âœ… Found {len(users)} users on Hacker News")
        return users[:limit]

    def _get_story_commenters(self, story_id: str, limit: int = 5) -> List[str]:
        """
        è·å–æŸä¸ªå¸–å­çš„è¯„è®ºè€…

        Args:
            story_id: å¸–å­ID
            limit: æ•°é‡é™åˆ¶

        Returns:
            ç”¨æˆ·ååˆ—è¡¨
        """
        commenters = []

        try:
            # è·å–å¸–å­è¯¦æƒ…
            response = requests.get(
                f"{self.api_base}/item/{story_id}.json",
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                story = response.json()
                kids = story.get('kids', [])[:limit]  # å–å‰Nä¸ªè¯„è®º

                for kid_id in kids:
                    # è·å–è¯„è®ºè¯¦æƒ…
                    comment_response = requests.get(
                        f"{self.api_base}/item/{kid_id}.json",
                        headers=self.headers,
                        timeout=30
                    )

                    if comment_response.status_code == 200:
                        comment = comment_response.json()
                        author = comment.get('by')
                        if author:
                            commenters.append(author)

                    time.sleep(0.2)

        except Exception as e:
            logger.debug(f"   Error getting commenters: {e}")

        return commenters

    def _get_user_data(self, username: str) -> Optional[Dict]:
        """
        è·å–ç”¨æˆ·æ•°æ®

        Args:
            username: HNç”¨æˆ·å

        Returns:
            ç”¨æˆ·æ•°æ®å­—å…¸
        """
        try:
            response = requests.get(
                f"{self.api_base}/user/{username}.json",
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()

                # åªè¿”å›karma > 100çš„ç”¨æˆ·ï¼ˆæ›´æ´»è·ƒ/å¯ä¿¡ï¼‰
                karma = data.get('karma', 0)
                if karma < 100:
                    return None

                user = {
                    'username': username,
                    'profile_url': f"https://news.ycombinator.com/user?id={username}",
                    'karma': karma,
                    'created': data.get('created', 0),
                    'about': data.get('about', ''),
                    'platform': 'hackernews'
                }

                return user

        except Exception as e:
            logger.debug(f"   Error getting user {username}: {e}")
            return None

    def get_user_profile(self, user_id: str) -> Dict:
        """
        è·å–Hacker Newsç”¨æˆ·è¯¦ç»†èµ„æ–™

        Args:
            user_id: ç”¨æˆ·å

        Returns:
            ç”¨æˆ·è¯¦ç»†èµ„æ–™
        """
        logger.debug(f"ğŸ“– Fetching HN profile: {user_id}")

        user_data = self._get_user_data(user_id)

        if user_data:
            return user_data
        else:
            return {
                'username': user_id,
                'profile_url': f"https://news.ycombinator.com/user?id={user_id}",
                'platform': 'hackernews',
                'status': 'not_found'
            }

    def extract_email(self, user_profile: Dict) -> Optional[str]:
        """
        ä»Hacker Newsèµ„æ–™æå–é‚®ç®±

        HNçš„aboutå­—æ®µæœ‰æ—¶åŒ…å«é‚®ç®±æˆ–è”ç³»æ–¹å¼

        Args:
            user_profile: ç”¨æˆ·èµ„æ–™

        Returns:
            é‚®ç®±åœ°å€æˆ–None
        """
        about = user_profile.get('about', '')

        if not about:
            return None

        # ç®€å•çš„é‚®ç®±æå–
        import re
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        matches = re.findall(email_pattern, about)

        if matches:
            return matches[0]

        return None

    def search_hiring_posts(self, limit: int = 10) -> List[Dict]:
        """
        æœç´¢"Who is hiring"å¸–å­ä¸­çš„å…¬å¸

        Hacker Newsæ¯æœˆæœ‰"Who is hiring"å¸–å­ï¼Œæ˜¯è·å–æ‹›è˜ä¿¡æ¯çš„å¥½åœ°æ–¹

        Args:
            limit: è¿”å›æ•°é‡

        Returns:
            æ‹›è˜ä¿¡æ¯åˆ—è¡¨
        """
        logger.info("ğŸ” Searching 'Who is hiring' posts...")

        hiring_posts = []

        try:
            # æœç´¢Who is hiringå¸–å­
            search_url = f"{self.algolia_base}/search"
            params = {
                'query': 'who is hiring',
                'tags': 'story',
                'hitsPerPage': 3
            }

            response = requests.get(
                search_url,
                params=params,
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                hits = data.get('hits', [])

                for hit in hits[:1]:  # åªå–æœ€æ–°çš„ä¸€ä¸ª
                    story_id = hit.get('objectID')
                    if story_id:
                        # è·å–è¯„è®ºï¼ˆæ¯ä¸ªè¯„è®ºæ˜¯ä¸€ä¸ªæ‹›è˜ä¿¡æ¯ï¼‰
                        hiring_posts = self._get_hiring_comments(story_id, limit)

        except Exception as e:
            logger.error(f"âŒ Error searching hiring posts: {e}")

        return hiring_posts

    def _get_hiring_comments(self, story_id: str, limit: int = 10) -> List[Dict]:
        """
        è·å–æ‹›è˜å¸–å­çš„è¯„è®º

        Args:
            story_id: å¸–å­ID
            limit: æ•°é‡é™åˆ¶

        Returns:
            æ‹›è˜ä¿¡æ¯åˆ—è¡¨
        """
        hiring_info = []

        try:
            response = requests.get(
                f"{self.api_base}/item/{story_id}.json",
                headers=self.headers,
                timeout=30
            )

            if response.status_code == 200:
                story = response.json()
                kids = story.get('kids', [])[:limit]

                for kid_id in kids:
                    comment_response = requests.get(
                        f"{self.api_base}/item/{kid_id}.json",
                        headers=self.headers,
                        timeout=30
                    )

                    if comment_response.status_code == 200:
                        comment = comment_response.json()
                        hiring_info.append({
                            'text': comment.get('text', ''),
                            'author': comment.get('by', ''),
                            'time': comment.get('time', 0)
                        })

                    time.sleep(0.2)

        except Exception as e:
            logger.debug(f"   Error getting hiring comments: {e}")

        return hiring_info


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    scraper = HackerNewsScraper()

    # æµ‹è¯•æœç´¢ç”¨æˆ·
    users = scraper.search_users(["startup", "founder"], limit=10)

    print(f"\nâœ… Found {len(users)} users:")
    for user in users:
        print(f"  - {user.get('username')} (karma: {user.get('karma', 0)})")
        print(f"    Profile: {user.get('profile_url')}")

    # æµ‹è¯•æœç´¢æ‹›è˜ä¿¡æ¯
    print("\nğŸ” Searching hiring posts...")
    hiring = scraper.search_hiring_posts(limit=5)
    print(f"âœ… Found {len(hiring)} hiring posts")
