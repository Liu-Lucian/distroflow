#!/usr/bin/env python3
"""
ç®€å•çš„Twitterç²‰ä¸çˆ¬è™«è„šæœ¬
Simple script to scrape Twitter followers
"""

import sys
import argparse
from src.twitter_scraper import TwitterWebScraper
import pandas as pd
from datetime import datetime


def main():
    parser = argparse.ArgumentParser(
        description='çˆ¬å–Twitterç²‰ä¸å¹¶æå–é‚®ç®± / Scrape Twitter followers and extract emails',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ Examples:
  # çˆ¬å–æŸä¸ªç”¨æˆ·çš„100ä¸ªç²‰ä¸
  python scrape_twitter.py elonmusk --count 100

  # çˆ¬å–å¹¶æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼ˆè°ƒè¯•ç”¨ï¼‰
  python scrape_twitter.py elonmusk --count 50 --show-browser

  # çˆ¬å–å¹¶åªæå–æœ‰é‚®ç®±çš„ç²‰ä¸
  python scrape_twitter.py elonmusk --count 200 --emails-only
        """
    )

    parser.add_argument(
        'username',
        help='è¦çˆ¬å–ç²‰ä¸çš„Twitterç”¨æˆ·å (Username to scrape followers from)'
    )

    parser.add_argument(
        '--count',
        type=int,
        default=100,
        help='è¦çˆ¬å–çš„ç²‰ä¸æ•°é‡ (Number of followers to scrape, default: 100)'
    )

    parser.add_argument(
        '--show-browser',
        action='store_true',
        help='æ˜¾ç¤ºæµè§ˆå™¨çª—å£ (Show browser window for debugging)'
    )

    parser.add_argument(
        '--emails-only',
        action='store_true',
        help='åªä¿å­˜æœ‰é‚®ç®±çš„ç²‰ä¸ (Only save followers with emails)'
    )

    parser.add_argument(
        '--output',
        default=None,
        help='è¾“å‡ºæ–‡ä»¶å (Output filename, default: auto-generated)'
    )

    args = parser.parse_args()

    print("=" * 60)
    print("Twitter Follower Scraper / Twitterç²‰ä¸çˆ¬è™«")
    print("=" * 60)
    print(f"Target: @{args.username}")
    print(f"Count: {args.count} followers")
    print("=" * 60)
    print()

    # åˆ›å»ºçˆ¬è™«
    scraper = TwitterWebScraper(headless=not args.show_browser)

    try:
        # çˆ¬å–ç²‰ä¸
        print("ğŸ” å¼€å§‹çˆ¬å–ç²‰ä¸... / Starting to scrape followers...")
        print()

        followers = scraper.get_followers(
            username=args.username,
            max_followers=args.count,
            extract_emails=True
        )

        if not followers:
            print("âŒ æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•ç²‰ä¸ / No followers scraped")
            return

        # åªä¿ç•™æœ‰é‚®ç®±çš„ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.emails_only:
            original_count = len(followers)
            followers = [f for f in followers if f.get('email')]
            print(f"\nğŸ“§ ç­›é€‰å‡ºæœ‰é‚®ç®±çš„ç²‰ä¸: {len(followers)}/{original_count}")

        # ç»Ÿè®¡
        emails_found = sum(1 for f in followers if f.get('email'))

        print()
        print("=" * 60)
        print("ç»“æœ / Results")
        print("=" * 60)
        print(f"âœ“ çˆ¬å–ç²‰ä¸æ•° / Followers scraped: {len(followers)}")
        print(f"âœ“ æ‰¾åˆ°é‚®ç®±æ•° / Emails found: {emails_found} ({emails_found/len(followers)*100:.1f}%)")
        print("=" * 60)
        print()

        # æ˜¾ç¤ºå‰5ä¸ªï¼ˆå¸¦é‚®ç®±çš„ï¼‰
        print("å‰å‡ ä¸ªç»“æœ / Sample results:")
        print("-" * 60)

        sample = [f for f in followers if f.get('email')][:5]
        if not sample:
            sample = followers[:5]

        for i, follower in enumerate(sample, 1):
            print(f"\n{i}. @{follower['username']}")
            print(f"   å§“å Name: {follower['name']}")
            if follower.get('email'):
                print(f"   ğŸ“§ é‚®ç®± Email: {follower['email']}")
            if follower.get('bio'):
                bio_preview = follower['bio'][:80] + '...' if len(follower['bio']) > 80 else follower['bio']
                print(f"   ç®€ä»‹ Bio: {bio_preview}")

        print()
        print("-" * 60)

        # å¯¼å‡ºæ•°æ®
        if not args.output:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"exports/twitter_{args.username}_followers_{timestamp}.csv"
        else:
            output_file = args.output

        # ç¡®ä¿exportsç›®å½•å­˜åœ¨
        import os
        os.makedirs('exports', exist_ok=True)

        # å¯¼å‡ºåˆ°CSV
        df = pd.DataFrame(followers)
        df.to_csv(output_file, index=False, encoding='utf-8-sig')

        print()
        print(f"âœ“ æ•°æ®å·²å¯¼å‡º / Data exported to: {output_file}")
        print()

        # æ˜¾ç¤ºExcelé¢„è§ˆæç¤º
        print("æç¤º / Tips:")
        print("â€¢ ä½¿ç”¨Excelæ‰“å¼€CSVæ–‡ä»¶æŸ¥çœ‹å®Œæ•´æ•°æ®")
        print("â€¢ Use Excel to open the CSV file for better viewing")
        print("â€¢ é‚®ç®±å¯ä»¥ç›´æ¥ç”¨äºé‚®ä»¶è¥é”€")
        print("â€¢ Emails can be used directly for email marketing")
        print()

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ / User interrupted")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯ / Error: {e}")
        import traceback
        traceback.print_exc()

    finally:
        scraper.close()
        print("\nâœ“ å®Œæˆ / Done!")


if __name__ == "__main__":
    main()
