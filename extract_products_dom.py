#!/usr/bin/env python3
"""
ä» Product Hunt é¡µé¢ DOM ç›´æ¥æå–äº§å“é“¾æ¥
"""
import sys
sys.path.insert(0, 'src')

from producthunt_commenter import ProductHuntCommenter
import os
import json
import time
from datetime import datetime

PRODUCT_LIST_FILE = "todays_producthunt_products.json"

def extract_products_from_dom(commenter: ProductHuntCommenter) -> list:
    """ä»é¡µé¢ DOM æå–äº§å“"""
    print("\nğŸ” ä» DOM æå–äº§å“...")

    try:
        # ç­‰å¾…é¡µé¢å……åˆ†åŠ è½½
        print("   â³ ç­‰å¾…äº§å“åŠ è½½...")
        time.sleep(5)

        # æ»šåŠ¨é¡µé¢ä»¥è§¦å‘åŠ è½½
        for i in range(3):
            commenter.page.evaluate("window.scrollTo(0, document.body.scrollHeight / 3 * " + str(i+1) + ")")
            time.sleep(2)
        commenter.page.evaluate("window.scrollTo(0, 0)")
        time.sleep(2)

        # ä½¿ç”¨ JavaScript æå–æ‰€æœ‰äº§å“ä¿¡æ¯
        js_code = """
        () => {
            const products = [];
            const seenSlugs = new Set();

            // æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            const links = document.querySelectorAll('a[href]');

            for (const link of links) {
                const href = link.getAttribute('href');

                // åªè¦åŒ…å« /posts/ çš„é“¾æ¥
                if (!href || !href.includes('/posts/')) continue;

                // è·³è¿‡ç‰¹æ®Šé¡µé¢
                if (href.includes('/posts/new') ||
                    href.includes('/posts/all') ||
                    href.includes('/posts/search')) {
                    continue;
                }

                // æå– slug
                const match = href.match(/\/posts\/([^/?#]+)/);
                if (!match) continue;

                const slug = match[1];
                if (seenSlugs.has(slug)) continue;
                seenSlugs.add(slug);

                // å°è¯•ä»é“¾æ¥é™„è¿‘çš„å…ƒç´ æå–åç§°å’Œæè¿°
                let name = '';
                let tagline = '';

                // å°è¯•å¤šç§æ–¹å¼è·å–äº§å“å
                name = link.textContent.trim();

                // å¦‚æœåç§°å¤ªé•¿ï¼Œå¯èƒ½åŒ…å«äº†æè¿°
                if (name.length > 50) {
                    name = name.substring(0, 50).trim();
                }

                // æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„æè¿°
                let parent = link.parentElement;
                for (let i = 0; i < 3 && parent; i++) {
                    const paragraphs = parent.querySelectorAll('p, div[class*="tagline"], div[class*="description"]');
                    for (const p of paragraphs) {
                        const text = p.textContent.trim();
                        if (text.length > 10 && text.length < 200 && !text.includes('votes') && !text.includes('comments')) {
                            tagline = text;
                            break;
                        }
                    }
                    if (tagline) break;
                    parent = parent.parentElement;
                }

                products.push({
                    slug: slug,
                    name: name || slug,
                    tagline: tagline
                });

                // é™åˆ¶æå–æ•°é‡
                if (products.length >= 20) break;
            }

            return products;
        }
        """

        raw_products = commenter.page.evaluate(js_code)
        print(f"   âœ… æå–åˆ° {len(raw_products)} ä¸ªäº§å“")

        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        products = []
        for p in raw_products:
            slug = p.get('slug', '')
            if not slug:
                continue

            name = p.get('name', slug)
            # æ¸…ç†åç§°
            if len(name) > 50:
                name = name[:50].strip()

            tagline = p.get('tagline', '')
            if not tagline:
                tagline = f"{name} - Product from Product Hunt"

            products.append({
                'url': f"https://www.producthunt.com/posts/{slug}",
                'name': name,
                'tagline': tagline,
                'category': 'Various',
                'description': tagline,
                'votes': 0
            })

        print(f"   âœ… æ ¼å¼åŒ– {len(products)} ä¸ªäº§å“")
        return products[:10]  # åªè¿”å›å‰10ä¸ª

    except Exception as e:
        print(f"   âŒ æå–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

def main():
    print("=" * 80)
    print("ğŸ” Product Hunt DOM æå–")
    print("=" * 80)

    commenter = ProductHuntCommenter()

    try:
        # è®¿é—®é¦–é¡µ
        print("\nğŸŒ è®¿é—® Product Hunt é¦–é¡µ...")
        commenter.setup_browser(headless=True)

        if not commenter.verify_login():
            print("âŒ ç™»å½•å¤±è´¥")
            return 1

        commenter.page.goto("https://www.producthunt.com", timeout=60000)
        print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
        time.sleep(10)

        # æå–äº§å“
        products = extract_products_from_dom(commenter)

        commenter.close_browser()

        if not products:
            print("\nâŒ æœªèƒ½æå–äº§å“")
            return 1

        # ä¿å­˜
        data = {
            "date": datetime.now().strftime('%Y-%m-%d'),
            "source": "DOM Extraction",
            "products": products,
            "updated_at": datetime.now().isoformat()
        }

        with open(PRODUCT_LIST_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… æˆåŠŸä¿å­˜ {len(products)} ä¸ªäº§å“")
        print("\nğŸ“‹ äº§å“åˆ—è¡¨:")
        for i, p in enumerate(products, 1):
            print(f"   {i}. {p['name']}")
            print(f"      {p['tagline'][:80]}...")
            print(f"      {p['url']}")

        print("\n" + "=" * 80)
        print("âœ… DOM æå–å®Œæˆï¼")
        print("=" * 80)

        return 0

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        try:
            commenter.close_browser()
        except:
            pass

if __name__ == "__main__":
    sys.exit(main())
