#!/usr/bin/env python3
"""
ä½¿ç”¨æ›´çœŸå®çš„æµè§ˆå™¨è¡Œä¸ºæ¥æå–äº§å“
å…³é”®ï¼šä¸ä½¿ç”¨ headlessæ¨¡å¼ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
"""
import sys
sys.path.insert(0, 'src')

from producthunt_commenter import ProductHuntCommenter
import os
import json
import time
from datetime import datetime
import random

PRODUCT_LIST_FILE = "todays_producthunt_products.json"

def human_like_scroll(page):
    """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º"""
    # éšæœºæ»šåŠ¨
    for i in range(5):
        scroll_amount = random.randint(200, 500)
        page.evaluate(f"window.scrollBy(0, {scroll_amount})")
        time.sleep(random.uniform(0.5, 1.5))

    # æ»šå›é¡¶éƒ¨
    page.evaluate("window.scrollTo(0, 0)")
    time.sleep(2)

def extract_products_realistic(commenter: ProductHuntCommenter) -> list:
    """ä½¿ç”¨çœŸå®æµè§ˆå™¨è¡Œä¸ºæå–äº§å“"""
    print("\nğŸ” ä½¿ç”¨çœŸå®æµè§ˆå™¨æå–äº§å“...")

    try:
        print("   â³ ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ˆ30ç§’ï¼‰...")
        time.sleep(30)  # é•¿æ—¶é—´ç­‰å¾…ï¼Œè®©æ‰€æœ‰å†…å®¹åŠ è½½

        # æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º
        print("   ğŸ“œ æ¨¡æ‹Ÿäººç±»æ»šåŠ¨...")
        human_like_scroll(commenter.page)

        # å†ç­‰ä¸€ä¼šå„¿
        time.sleep(5)

        # å°è¯•æå–é“¾æ¥
        js_code = """
        () => {
            const allLinks = Array.from(document.querySelectorAll('a[href]'));
            const postLinks = allLinks
                .map(a => ({
                    href: a.getAttribute('href'),
                    text: a.textContent.trim()
                }))
                .filter(item => {
                    const href = item.href;
                    return href &&
                           href.includes('/posts/') &&
                           !href.includes('/posts/new') &&
                           !href.includes('/posts/all') &&
                           !href.includes('/posts?') &&
                           item.text.length > 2 &&
                           item.text.length < 100;
                });

            // å»é‡
            const seen = new Set();
            const unique = [];
            for (const item of postLinks) {
                const url = item.href.startsWith('/') ?
                    'https://www.producthunt.com' + item.href :
                    item.href;
                const cleanUrl = url.split('?')[0].split('#')[0];

                if (!seen.has(cleanUrl)) {
                    seen.add(cleanUrl);
                    unique.push({
                        url: cleanUrl,
                        name: item.text
                    });
                }

                if (unique.length >= 15) break;
            }

            return {
                totalLinks: allLinks.length,
                postLinks: postLinks.length,
                unique: unique
            };
        }
        """

        result = commenter.page.evaluate(js_code)
        print(f"\n   ğŸ“Š ç»Ÿè®¡:")
        print(f"      æ€»é“¾æ¥æ•°: {result['totalLinks']}")
        print(f"      /posts/ é“¾æ¥æ•°: {result['postLinks']}")
        print(f"      å»é‡å: {len(result['unique'])}")

        if not result['unique']:
            print("   âŒ æœªæ‰¾åˆ°äº§å“é“¾æ¥")

            # æˆªå›¾ä¿å­˜çŠ¶æ€
            commenter.page.screenshot(path="debug_no_products.png")
            print("   ğŸ“¸ å·²ä¿å­˜è°ƒè¯•æˆªå›¾: debug_no_products.png")

            # ä¿å­˜HTML
            with open("debug_no_products.html", 'w', encoding='utf-8') as f:
                f.write(commenter.page.content())
            print("   ğŸ“„ å·²ä¿å­˜HTML: debug_no_products.html")

            return []

        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        products = []
        for item in result['unique'][:10]:
            url = item['url']
            name = item['name'][:50]

            products.append({
                'url': url,
                'name': name,
                'tagline': f'{name} - Product from Product Hunt Today',
                'category': 'Various',
                'description': 'Auto-extracted product',
                'votes': 0
            })

        print(f"   âœ… æå–åˆ° {len(products)} ä¸ªäº§å“\n")

        # æ˜¾ç¤ºäº§å“åˆ—è¡¨
        for i, p in enumerate(products, 1):
            print(f"      {i}. {p['name']}")
            print(f"         {p['url']}")

        return products

    except Exception as e:
        print(f"   âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []

def main():
    print("=" * 80)
    print("ğŸ” Product Hunt çœŸå®æµè§ˆå™¨æå–ï¼ˆæœ€åå°è¯•ï¼‰")
    print("=" * 80)
    print("\nâš ï¸  å°†æ‰“å¼€å¯è§æµè§ˆå™¨çª—å£ï¼Œè¯·å‹¿å…³é—­ï¼")
    print("   éœ€è¦ç­‰å¾…çº¦ 40 ç§’å®Œæˆæå–...\n")

    commenter = ProductHuntCommenter()

    try:
        print("ğŸŒ å¯åŠ¨æµè§ˆå™¨...")
        commenter.setup_browser(headless=False)  # ä½¿ç”¨å¯è§æµè§ˆå™¨ï¼

        if not commenter.verify_login():
            print("âŒ ç™»å½•å¤±è´¥")
            return 1

        print("ğŸŒ è®¿é—® Product Hunt...")
        commenter.page.goto("https://www.producthunt.com", timeout=60000)

        # æå–äº§å“
        products = extract_products_realistic(commenter)

        print("\nâ¸ï¸  æŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨å¹¶ä¿å­˜ç»“æœ...")
        input()

        commenter.close_browser()

        if not products:
            print("\nâŒ æœªèƒ½æå–äº§å“")
            return 1

        # ä¿å­˜
        data = {
            "date": datetime.now().strftime('%Y-%m-%d'),
            "source": "Realistic Browser Extraction",
            "products": products,
            "updated_at": datetime.now().isoformat()
        }

        with open(PRODUCT_LIST_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"\n" + "=" * 80)
        print(f"âœ… æˆåŠŸä¿å­˜ {len(products)} ä¸ªäº§å“åˆ° {PRODUCT_LIST_FILE}")
        print("=" * 80)

        return 0

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        try:
            commenter.close_browser()
        except:
            pass

if __name__ == "__main__":
    sys.exit(main())
