#!/usr/bin/env python3
"""
æµ‹è¯•å¤šå¹³å°é›†æˆ - Test Multi-Platform Integration
æµ‹è¯•LinkedInå’ŒGitHub scrapersæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
from pathlib import Path

# Add src directory to Python path
SCRIPT_DIR = Path(__file__).parent.absolute()
sys.path.insert(0, str(SCRIPT_DIR / "src"))
sys.path.insert(0, str(SCRIPT_DIR))

from src.linkedin_scraper import LinkedInScraper
from src.github_scraper import GitHubScraper


def test_linkedin():
    """æµ‹è¯•LinkedIn scraper"""
    print("\n" + "="*70)
    print("ğŸ”µ Testing LinkedIn Scraper")
    print("="*70)

    try:
        scraper = LinkedInScraper(
            auth_file=str(SCRIPT_DIR / "linkedin_auth.json")
        )
        print("âœ… LinkedIn scraper initialized")

        # æµ‹è¯•æœç´¢
        keywords = ["recruiter", "hiring manager"]
        print(f"\nğŸ” Searching for: {', '.join(keywords)} (limit: 3)")

        users = scraper.search_users(keywords, limit=3)
        print(f"âœ… Found {len(users)} users")

        # æ˜¾ç¤ºç”¨æˆ·
        for i, user in enumerate(users, 1):
            print(f"\n  {i}. {user.get('name', 'N/A')}")
            print(f"     Profile: {user.get('profile_url', 'N/A')}")
            print(f"     Headline: {user.get('headline', 'N/A')}")
            print(f"     Location: {user.get('location', 'N/A')}")

        # æµ‹è¯•è·å–è¯¦ç»†èµ„æ–™
        if users:
            print(f"\nğŸ“– Getting detailed profile for first user...")
            profile = scraper.get_user_profile(users[0]['profile_url'])

            print(f"  Name: {profile.get('name', 'N/A')}")
            print(f"  Headline: {profile.get('headline', 'N/A')}")
            print(f"  Company: {profile.get('company', 'N/A')}")
            print(f"  Location: {profile.get('location', 'N/A')}")
            print(f"  Email: {profile.get('email', 'Not found')}")

        # å…³é—­æµè§ˆå™¨
        scraper._close_browser()
        print("\nâœ… LinkedIn test completed")

    except Exception as e:
        print(f"\nâŒ LinkedIn test failed: {e}")
        import traceback
        traceback.print_exc()


def test_github():
    """æµ‹è¯•GitHub scraper"""
    print("\n" + "="*70)
    print("ğŸ”µ Testing GitHub Scraper")
    print("="*70)

    try:
        scraper = GitHubScraper(
            auth_file=str(SCRIPT_DIR / "platforms_auth.json")
        )
        print("âœ… GitHub scraper initialized")

        # æµ‹è¯•æœç´¢
        keywords = ["recruiter", "hiring"]
        print(f"\nğŸ” Searching for: {', '.join(keywords)} (limit: 3)")

        users = scraper.search_users(keywords, limit=3)
        print(f"âœ… Found {len(users)} users")

        # æ˜¾ç¤ºç”¨æˆ·
        for i, user in enumerate(users, 1):
            print(f"\n  {i}. @{user.get('username', 'N/A')}")
            print(f"     Profile: {user.get('profile_url', 'N/A')}")

        # æµ‹è¯•è·å–è¯¦ç»†èµ„æ–™
        if users:
            print(f"\nğŸ“– Getting detailed profile for first user...")
            profile = scraper.get_user_profile(users[0]['username'])

            print(f"  Username: @{profile.get('username', 'N/A')}")
            print(f"  Name: {profile.get('name', 'N/A')}")
            print(f"  Bio: {profile.get('bio', 'N/A')[:100]}")
            print(f"  Company: {profile.get('company', 'N/A')}")
            print(f"  Location: {profile.get('location', 'N/A')}")
            print(f"  Email: {profile.get('email', 'Not public')}")
            print(f"  Followers: {profile.get('followers_count', 0)}")
            print(f"  Public repos: {profile.get('public_repos', 0)}")

        print("\nâœ… GitHub test completed")

    except Exception as e:
        print(f"\nâŒ GitHub test failed: {e}")
        import traceback
        traceback.print_exc()


def test_integration():
    """æµ‹è¯•å®Œæ•´é›†æˆæµç¨‹"""
    print("\n" + "="*70)
    print("ğŸ”µ Testing Full Integration")
    print("="*70)

    # æµ‹è¯•LinkedInçš„å®Œæ•´æµç¨‹
    print("\n1ï¸âƒ£  Testing LinkedIn full flow (search + profile + email)...")
    try:
        scraper = LinkedInScraper(
            auth_file=str(SCRIPT_DIR / "linkedin_auth.json")
        )

        # ä½¿ç”¨get_leadsæ–¹æ³•ï¼ˆç»§æ‰¿è‡ªbase classï¼‰
        leads = scraper.get_leads(keywords=["recruiter"], limit=2)

        print(f"   âœ… Got {len(leads)} leads")
        for lead in leads:
            print(f"   - {lead.get('name', 'N/A')}: {lead.get('email', 'No email')}")

        scraper._close_browser()

    except Exception as e:
        print(f"   âŒ Error: {e}")

    # æµ‹è¯•GitHubçš„å®Œæ•´æµç¨‹
    print("\n2ï¸âƒ£  Testing GitHub full flow (search + profile + email)...")
    try:
        scraper = GitHubScraper(
            auth_file=str(SCRIPT_DIR / "platforms_auth.json")
        )

        # ä½¿ç”¨get_leadsæ–¹æ³•
        leads = scraper.get_leads(keywords=["interview", "hiring"], limit=2)

        print(f"   âœ… Got {len(leads)} leads")
        for lead in leads:
            print(f"   - @{lead.get('username', 'N/A')}: {lead.get('email', 'No email')}")

    except Exception as e:
        print(f"   âŒ Error: {e}")


if __name__ == "__main__":
    print("\nğŸ§ª Multi-Platform Integration Test")
    print("="*70)

    # é€‰æ‹©æµ‹è¯•
    import argparse
    parser = argparse.ArgumentParser(description='Test platform scrapers')
    parser.add_argument(
        '--platform',
        choices=['linkedin', 'github', 'all'],
        default='all',
        help='Which platform to test'
    )

    args = parser.parse_args()

    if args.platform in ['linkedin', 'all']:
        test_linkedin()

    if args.platform in ['github', 'all']:
        test_github()

    if args.platform == 'all':
        test_integration()

    print("\n" + "="*70)
    print("ğŸ‰ All tests completed!")
    print("="*70)
