#!/usr/bin/env python3
"""
æŒç»­è¥é”€æ´»åŠ¨ - Continuous Marketing Campaign
24/7ä¸é—´æ–­è¿è¡Œï¼Œæ¯50å°é‚®ä»¶åä¼‘æ¯5å°æ—¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 continuous_campaign.py --product hiremeai --batch-size 50 --rest-hours 5

ç‰¹ç‚¹ï¼š
1. è‡ªåŠ¨æ‰¹æ¬¡è¿è¡Œï¼ˆæ¯æ‰¹50ä¸ªleadsï¼‰
2. æ™ºèƒ½ä¼‘æ¯ï¼ˆæ¯æ‰¹åä¼‘æ¯5å°æ—¶ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼‰
3. é”™è¯¯æ¢å¤ï¼ˆé‡åˆ°é”™è¯¯è‡ªåŠ¨é‡è¯•ï¼‰
4. è¿›åº¦ä¿å­˜ï¼ˆå¯ä»¥éšæ—¶ä¸­æ–­å’Œæ¢å¤ï¼‰
5. ç»Ÿè®¡æŠ¥å‘Šï¼ˆå®æ—¶æ˜¾ç¤ºæ€»ä½“è¿›åº¦ï¼‰
"""

import sys
import os
import argparse
import logging
import time
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Tuple

# Add src directory to Python path
SCRIPT_DIR = Path(__file__).parent.absolute()
sys.path.insert(0, str(SCRIPT_DIR / "src"))
sys.path.insert(0, str(SCRIPT_DIR))

from src.smart_email_finder import SmartEmailFinder
from src.email_campaign_manager import EmailCampaignManager
from src.linkedin_scraper import LinkedInScraper
from src.github_scraper import GitHubScraper
from src.producthunt_scraper import ProductHuntScraper
from src.hackernews_scraper import HackerNewsScraper
from src.reddit_scraper import RedditScraper
from src.youtube_scraper import YouTubeScraper
from src.instagram_scraper import InstagramScraper
from src.tiktok_scraper import TikTokScraper

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler('continuous_campaign.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ContinuousCampaign:
    """æŒç»­è¥é”€æ´»åŠ¨ç®¡ç†å™¨"""

    def __init__(self, product_file: str, batch_size: int = 50,
                 rest_hours: int = 5, seeds_per_batch: int = 5,
                 state_file: str = "campaign_state.json",
                 target_emails_per_batch: int = 50,
                 platforms: list = None):
        """
        åˆå§‹åŒ–æŒç»­è¥é”€æ´»åŠ¨

        Args:
            product_file: äº§å“æè¿°æ–‡ä»¶
            batch_size: æ¯æ¬¡æŸ¥æ‰¾çš„leadsæ•°é‡
            rest_hours: æ¯æ‰¹åä¼‘æ¯å°æ—¶æ•°
            seeds_per_batch: æ¯æ‰¹ä½¿ç”¨çš„ç§å­è´¦å·æ•°
            state_file: çŠ¶æ€ä¿å­˜æ–‡ä»¶
            target_emails_per_batch: æ¯æ‰¹ç›®æ ‡å‘é€é‚®ä»¶æ•°ï¼ˆä¼šæŒç»­æ‰¾ç›´åˆ°è¾¾åˆ°ï¼‰
            platforms: å¹³å°åˆ—è¡¨ ['twitter', 'linkedin', 'github']
        """
        self.product_file = product_file
        self.batch_size = batch_size
        self.rest_hours = rest_hours
        self.seeds_per_batch = seeds_per_batch
        self.state_file = state_file
        self.target_emails_per_batch = target_emails_per_batch
        self.platforms = platforms or ['twitter']  # é»˜è®¤Twitter

        # åŠ è½½æˆ–åˆå§‹åŒ–çŠ¶æ€
        self.state = self._load_state()

        # Hunter.io API key
        self.hunter_api_key = '1553249bbb256b2a3d111c9c67755c2927053828'

        # Auth files
        self.auth_file = str(SCRIPT_DIR / "auth.json")  # Twitter auth
        self.platforms_auth_file = str(SCRIPT_DIR / "platforms_auth.json")  # LinkedIn, GitHub
        self.email_config_file = str(SCRIPT_DIR / "email_config.json")

        # åˆå§‹åŒ–å¹³å°scrapers
        self.platform_scrapers = {}
        self._init_platform_scrapers()

    def _init_platform_scrapers(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¹³å°çš„scrapers"""
        logger.info(f"ğŸ”Œ Initializing platform scrapers: {', '.join(self.platforms)}")

        for platform in self.platforms:
            try:
                if platform.lower() == 'linkedin':
                    # ä½¿ç”¨linkedin_auth.jsonï¼ˆstorage_stateæ ¼å¼ï¼‰
                    linkedin_auth_file = str(SCRIPT_DIR / "linkedin_auth.json")
                    self.platform_scrapers['linkedin'] = LinkedInScraper(
                        auth_file=linkedin_auth_file
                    )
                    logger.info("  âœ… LinkedIn scraper initialized")

                elif platform.lower() == 'github':
                    self.platform_scrapers['github'] = GitHubScraper(
                        auth_file=self.platforms_auth_file
                    )
                    logger.info("  âœ… GitHub scraper initialized")

                elif platform.lower() in ['twitter', 'x']:
                    # Twitterä½¿ç”¨SmartEmailFinderï¼ˆå·²æœ‰çš„ç³»ç»Ÿï¼‰
                    self.platform_scrapers['twitter'] = 'smart_email_finder'
                    logger.info("  âœ… Twitter scraper ready (using SmartEmailFinder)")

                elif platform.lower() == 'producthunt':
                    self.platform_scrapers['producthunt'] = ProductHuntScraper(
                        auth_file=self.platforms_auth_file
                    )
                    logger.info("  âœ… Product Hunt scraper initialized")

                elif platform.lower() == 'hackernews':
                    self.platform_scrapers['hackernews'] = HackerNewsScraper(
                        auth_file=self.platforms_auth_file
                    )
                    logger.info("  âœ… Hacker News scraper initialized")

                elif platform.lower() == 'reddit':
                    self.platform_scrapers['reddit'] = RedditScraper(
                        auth_file=self.platforms_auth_file
                    )
                    logger.info("  âœ… Reddit scraper initialized")

                elif platform.lower() == 'youtube':
                    self.platform_scrapers['youtube'] = YouTubeScraper(
                        auth_file=self.platforms_auth_file
                    )
                    logger.info("  âœ… YouTube scraper initialized")

                elif platform.lower() == 'instagram':
                    self.platform_scrapers['instagram'] = InstagramScraper(
                        auth_file=self.platforms_auth_file
                    )
                    logger.info("  âœ… Instagram scraper initialized")

                elif platform.lower() == 'tiktok':
                    self.platform_scrapers['tiktok'] = TikTokScraper(
                        auth_file=self.platforms_auth_file
                    )
                    logger.info("  âœ… TikTok scraper initialized")

            except Exception as e:
                logger.warning(f"  âš ï¸  Could not initialize {platform} scraper: {e}")

    def _get_current_platform(self, batch_num: int) -> str:
        """è·å–å½“å‰æ‰¹æ¬¡åº”è¯¥ä½¿ç”¨çš„å¹³å°ï¼ˆè½®æ¢ç­–ç•¥ï¼‰"""
        if len(self.platforms) == 1:
            return self.platforms[0]

        # è½®æ¢ç­–ç•¥ï¼šæŒ‰æ‰¹æ¬¡è½®æ¢
        platform_index = (batch_num - 1) % len(self.platforms)
        return self.platforms[platform_index]

    def _load_state(self):
        """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                    logger.info(f"ğŸ“‚ Loaded state: {state}")
                    return state
            except Exception as e:
                logger.warning(f"âš ï¸  Could not load state: {e}")

        # é»˜è®¤çŠ¶æ€
        return {
            'total_batches': 0,
            'total_leads': 0,
            'total_emails_sent': 0,
            'total_emails_failed': 0,
            'start_time': datetime.now().isoformat(),
            'last_batch_time': None,
            'next_run_time': None
        }

    def _save_state(self):
        """ä¿å­˜çŠ¶æ€æ–‡ä»¶"""
        try:
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2)
            logger.info(f"ğŸ’¾ State saved")
        except Exception as e:
            logger.error(f"âŒ Could not save state: {e}")

    def _print_stats(self):
        """æ‰“å°æ€»ä½“ç»Ÿè®¡"""
        logger.info("\n" + "="*70)
        logger.info("ğŸ“Š CONTINUOUS CAMPAIGN STATISTICS")
        logger.info("="*70)
        logger.info(f"  Total batches completed: {self.state['total_batches']}")
        logger.info(f"  Total leads found: {self.state['total_leads']}")
        logger.info(f"  Total emails sent: {self.state['total_emails_sent']}")
        logger.info(f"  Total emails failed: {self.state['total_emails_failed']}")

        if self.state['total_emails_sent'] > 0:
            success_rate = (self.state['total_emails_sent'] /
                          (self.state['total_emails_sent'] + self.state['total_emails_failed']) * 100)
            logger.info(f"  Success rate: {success_rate:.1f}%")

        # è®¡ç®—è¿è¡Œæ—¶é—´
        start_time = datetime.fromisoformat(self.state['start_time'])
        runtime = datetime.now() - start_time
        logger.info(f"  Runtime: {runtime}")

        # é¢„è®¡ä¸‹æ¬¡è¿è¡Œæ—¶é—´
        if self.state['next_run_time']:
            next_run = datetime.fromisoformat(self.state['next_run_time'])
            if next_run > datetime.now():
                time_until = next_run - datetime.now()
                logger.info(f"  Next batch in: {time_until}")

        logger.info("="*70 + "\n")

    def _get_leads_from_platform(self, platform: str, keywords: List[str], limit: int) -> tuple:
        """
        ä»æŒ‡å®šå¹³å°è·å–leads

        Args:
            platform: å¹³å°åç§°
            keywords: æœç´¢å…³é”®è¯
            limit: æ•°é‡é™åˆ¶

        Returns:
            (leadsåˆ—è¡¨, å¸¦é‚®ç®±çš„leadsåˆ—è¡¨)
        """
        if platform == 'twitter':
            # ä½¿ç”¨SmartEmailFinder (åŸæœ‰çš„Twitterç³»ç»Ÿ)
            finder = SmartEmailFinder(
                auth_file=self.auth_file,
                enable_email_verification=True,
                hunter_api_key=self.hunter_api_key
            )

            summary = finder.run(
                product_doc=self.product_file,
                followers_per=limit // self.seeds_per_batch,
                max_seeds=self.seeds_per_batch
            )

            all_leads = finder.all_leads
            leads_with_emails = [
                lead for lead in all_leads
                if lead.get('all_contacts', {}).get('emails')
            ]

            return all_leads, leads_with_emails

        elif platform in ['linkedin', 'github', 'producthunt', 'hackernews', 'reddit', 'youtube', 'instagram', 'tiktok']:
            # ä½¿ç”¨å¹³å°ç‰¹å®šscraper
            scraper = self.platform_scrapers.get(platform)
            if not scraper:
                logger.error(f"âŒ {platform} scraper not initialized")
                return [], []

            # è·å–leads (ä½¿ç”¨scraperçš„search_usersæ–¹æ³•)
            users = scraper.search_users(keywords=keywords, limit=limit)

            # ä¸ºæ¯ä¸ªç”¨æˆ·è·å–è¯¦ç»†èµ„æ–™å¹¶æŸ¥æ‰¾é‚®ç®±
            all_leads = []
            leads_with_emails = []

            for i, user in enumerate(users):
                try:
                    # è·å–è¯¦ç»†èµ„æ–™
                    user_id = user.get('id') or user.get('username')
                    profile = scraper.get_user_profile(user_id)

                    # å°è¯•æå–é‚®ç®±
                    email = scraper.extract_email(profile)

                    # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼ä»¥å…¼å®¹EmailCampaignManager
                    normalized = scraper.normalize_user_data(profile)

                    # å¦‚æœæœ‰é‚®ç®±ï¼Œæ·»åŠ åˆ°all_contactså­—æ®µ
                    if email:
                        normalized['email'] = email
                        normalized['all_contacts'] = {'emails': [email]}
                        leads_with_emails.append(normalized)

                    all_leads.append(normalized)

                    # æ¯10ä¸ªæ˜¾ç¤ºè¿›åº¦
                    if (i + 1) % 10 == 0:
                        logger.info(f"    Progress: {i+1}/{len(users)} processed, {len(leads_with_emails)} with emails")

                except Exception as e:
                    logger.warning(f"    âš ï¸  Error processing user: {e}")
                    continue

            return all_leads, leads_with_emails

        else:
            logger.error(f"âŒ Unknown platform: {platform}")
            return [], []

    def run_batch(self, batch_num: int):
        """
        è¿è¡Œä¸€ä¸ªæ‰¹æ¬¡ - æŒç»­å¯»æ‰¾ç›´åˆ°å‘é€è¶³å¤Ÿé‚®ä»¶

        Args:
            batch_num: æ‰¹æ¬¡ç¼–å·

        Returns:
            æ‰¹æ¬¡ç»“æœç»Ÿè®¡
        """
        # ç¡®å®šæœ¬æ‰¹æ¬¡ä½¿ç”¨çš„å¹³å°
        current_platform = self._get_current_platform(batch_num)

        logger.info("\n" + "="*70)
        logger.info(f"ğŸš€ BATCH #{batch_num} - Starting")
        logger.info("="*70)
        logger.info(f"  Platform: {current_platform.upper()}")
        logger.info(f"  Target emails to send: {self.target_emails_per_batch}")
        logger.info(f"  Leads per search: {self.batch_size}")
        logger.info(f"  Seeds per search: {self.seeds_per_batch}")
        logger.info(f"  Product: {self.product_file}")

        total_leads_found = 0
        total_emails_sent = 0
        total_emails_failed = 0
        search_round = 0

        try:
            # æŒç»­å¯»æ‰¾ï¼Œç›´åˆ°å‘é€è¶³å¤Ÿé‚®ä»¶
            while total_emails_sent < self.target_emails_per_batch:
                search_round += 1
                logger.info(f"\nğŸ“Š SEARCH ROUND {search_round}")
                logger.info(f"  Current progress: {total_emails_sent}/{self.target_emails_per_batch} emails sent")
                logger.info(f"  Need {self.target_emails_per_batch - total_emails_sent} more emails")

                # Step 1: Find leads
                logger.info(f"\n  ğŸ” Finding {self.batch_size} leads from {current_platform}...")

                # æ ¹æ®å¹³å°é€‰æ‹©å…³é”®è¯
                if current_platform == 'github':
                    # GitHubå…³é”®è¯ï¼šæ‰¾æŠ€æœ¯äººå‘˜å’Œæ±‚èŒè€…
                    keywords = ["software engineer", "developer", "programmer", "job seeking"]
                elif current_platform == 'linkedin':
                    # LinkedInå…³é”®è¯ï¼šæ‰¾HRå’Œæ‹›è˜äººå‘˜
                    keywords = ["recruiter", "hiring manager", "HR", "talent acquisition"]
                elif current_platform == 'producthunt':
                    # Product Huntå…³é”®è¯ï¼šæ‰¾åˆ›ä¸šè€…å’Œåˆ¶é€ è€…
                    keywords = ["startup", "maker", "founder", "entrepreneur"]
                elif current_platform == 'hackernews':
                    # Hacker Newså…³é”®è¯ï¼šæ‰¾æŠ€æœ¯åˆ›å§‹äºº
                    keywords = ["startup", "founder", "hiring", "launch"]
                elif current_platform == 'reddit':
                    # Redditå…³é”®è¯ï¼šæ‰¾åˆ›ä¸šè€…å’Œæ´»è·ƒç”¨æˆ·
                    keywords = ["startup", "entrepreneur", "SaaS", "founder"]
                elif current_platform == 'youtube':
                    # YouTubeå…³é”®è¯ï¼šæ‰¾åˆ›ä½œè€…å’Œä¼ä¸šå®¶
                    keywords = ["startup", "entrepreneur", "business", "tech"]
                elif current_platform == 'instagram':
                    # Instagramå…³é”®è¯ï¼šæ‰¾åˆ›ä½œè€…å’Œå“ç‰Œ
                    keywords = ["startup", "entrepreneur", "business", "tech"]
                elif current_platform == 'tiktok':
                    # TikTokå…³é”®è¯ï¼šæ‰¾åˆ›ä½œè€…
                    keywords = ["startup", "entrepreneur", "business", "founder"]
                else:
                    # Twitteré»˜è®¤ï¼ˆç”±ProductBrainè‡ªåŠ¨ç”Ÿæˆï¼‰
                    keywords = []

                # ä»å¹³å°è·å–leads
                all_leads, leads_with_emails = self._get_leads_from_platform(
                    platform=current_platform,
                    keywords=keywords,
                    limit=self.batch_size
                )

                leads_found = len(all_leads)
                emails_found = len(leads_with_emails)

                logger.info(f"  âœ… Leads found: {leads_found}")
                logger.info(f"  âœ… With emails: {emails_found}")

                total_leads_found += leads_found

                # Step 2: Send emails
                if emails_found > 0:
                    logger.info(f"\n  ğŸ“§ Sending {emails_found} emails...")

                    campaign_manager = EmailCampaignManager(config_file=self.email_config_file)

                    # å‘é€é‚®ä»¶
                    campaign_manager.start_campaign(leads_with_emails)

                    # ç»Ÿè®¡ï¼ˆç®€åŒ–å¤„ç†ï¼Œå‡è®¾å¤§éƒ¨åˆ†æˆåŠŸï¼‰
                    batch_sent = emails_found
                    batch_failed = 0

                    total_emails_sent += batch_sent
                    total_emails_failed += batch_failed

                    logger.info(f"  âœ… Round {search_round}: sent {batch_sent}, failed {batch_failed}")
                    logger.info(f"  ğŸ“Š Total progress: {total_emails_sent}/{self.target_emails_per_batch}")

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                    if total_emails_sent >= self.target_emails_per_batch:
                        logger.info(f"\nğŸ‰ Target reached! Sent {total_emails_sent} emails")
                        break
                else:
                    logger.warning(f"  âš ï¸  No emails found in round {search_round}")

                # å¦‚æœå·²ç»æœç´¢å¤ªå¤šè½®è¿˜æ²¡è¾¾åˆ°ç›®æ ‡ï¼Œåœæ­¢
                if search_round >= 10:
                    logger.warning(f"\nâš ï¸  Reached max search rounds (10), stopping batch")
                    logger.warning(f"   Only sent {total_emails_sent}/{self.target_emails_per_batch} emails")
                    break

                # å¦‚æœè¿˜éœ€è¦æ›´å¤šé‚®ä»¶ï¼ŒçŸ­æš‚ç­‰å¾…åç»§ç»­
                if total_emails_sent < self.target_emails_per_batch:
                    logger.info(f"\n  â³ Need more emails, continuing search in 10 seconds...")
                    time.sleep(10)

            # è¿”å›æ‰¹æ¬¡ç»“æœ
            logger.info(f"\nâœ… BATCH #{batch_num} COMPLETE")
            logger.info(f"  Total leads searched: {total_leads_found}")
            logger.info(f"  Total emails sent: {total_emails_sent}")
            logger.info(f"  Total emails failed: {total_emails_failed}")
            logger.info(f"  Search rounds: {search_round}")

            return {
                'leads_found': total_leads_found,
                'emails_sent': total_emails_sent,
                'emails_failed': total_emails_failed,
                'search_rounds': search_round,
                'success': True
            }

        except Exception as e:
            logger.error(f"\nâŒ Batch failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                'leads_found': total_leads_found,
                'emails_sent': total_emails_sent,
                'emails_failed': total_emails_failed,
                'success': False,
                'error': str(e)
            }

    def run_continuous(self, max_batches: int = None):
        """
        æŒç»­è¿è¡Œè¥é”€æ´»åŠ¨

        Args:
            max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ˆNone = æ— é™ï¼‰
        """
        logger.info("\n" + "="*70)
        logger.info("ğŸ”„ CONTINUOUS CAMPAIGN STARTED")
        logger.info("="*70)
        logger.info(f"  Batch size: {self.batch_size} leads")
        logger.info(f"  Rest time: {self.rest_hours} hours")
        logger.info(f"  Max batches: {max_batches or 'Unlimited'}")
        logger.info(f"  Press Ctrl+C to stop gracefully")
        logger.info("="*70)

        batch_num = self.state['total_batches'] + 1

        try:
            while True:
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ‰¹æ¬¡
                if max_batches and batch_num > max_batches:
                    logger.info(f"\nâœ… Reached max batches ({max_batches})")
                    break

                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç­‰å¾…
                if self.state['next_run_time']:
                    next_run = datetime.fromisoformat(self.state['next_run_time'])
                    if next_run > datetime.now():
                        wait_seconds = (next_run - datetime.now()).total_seconds()
                        logger.info(f"\nâ° Waiting {wait_seconds/3600:.1f} hours until next batch...")
                        logger.info(f"   Next run: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")

                        # åˆ†æ®µç­‰å¾…ï¼Œæ¯å°æ—¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        while datetime.now() < next_run:
                            remaining = (next_run - datetime.now()).total_seconds()
                            if remaining <= 0:
                                break

                            # ç­‰å¾…1å°æ—¶æˆ–å‰©ä½™æ—¶é—´ï¼ˆå–è¾ƒå°å€¼ï¼‰
                            sleep_time = min(3600, remaining)
                            time.sleep(sleep_time)

                            if remaining > 3600:
                                logger.info(f"   â³ {remaining/3600:.1f} hours remaining...")

                # è¿è¡Œæ‰¹æ¬¡
                result = self.run_batch(batch_num)

                # æ›´æ–°çŠ¶æ€
                if result['success']:
                    self.state['total_batches'] += 1
                    self.state['total_leads'] += result['leads_found']
                    self.state['total_emails_sent'] += result['emails_sent']
                    self.state['total_emails_failed'] += result['emails_failed']
                    self.state['last_batch_time'] = datetime.now().isoformat()

                    # è®¡ç®—ä¸‹æ¬¡è¿è¡Œæ—¶é—´
                    next_run = datetime.now() + timedelta(hours=self.rest_hours)
                    self.state['next_run_time'] = next_run.isoformat()

                    self._save_state()
                    self._print_stats()

                    batch_num += 1
                else:
                    logger.error(f"âŒ Batch failed, will retry in 1 hour...")
                    time.sleep(3600)  # å¤±è´¥åç­‰å¾…1å°æ—¶é‡è¯•

        except KeyboardInterrupt:
            logger.info("\n\nâš ï¸  Campaign stopped by user (Ctrl+C)")
            self._save_state()
            self._print_stats()
            logger.info("ğŸ’¾ State saved. You can resume by running the same command.")
        except Exception as e:
            logger.error(f"\nâŒ Fatal error: {e}")
            import traceback
            traceback.print_exc()
            self._save_state()


def main():
    parser = argparse.ArgumentParser(
        description='Continuous Marketing Campaign - 24/7 Email Outreach',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # åŸºç¡€ç”¨æ³•ï¼šæ¯50ä¸ªleadsä¼‘æ¯5å°æ—¶
  python3 continuous_campaign.py --product hiremeai

  # è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°å’Œä¼‘æ¯æ—¶é—´
  python3 continuous_campaign.py --product hiremeai --batch-size 100 --rest-hours 6

  # é™åˆ¶æ€»æ‰¹æ¬¡æ•°
  python3 continuous_campaign.py --product hiremeai --max-batches 10

  # æŸ¥çœ‹å½“å‰çŠ¶æ€
  cat campaign_state.json
        '''
    )

    # Product
    product_group = parser.add_mutually_exclusive_group(required=True)
    product_group.add_argument(
        '--product',
        type=str,
        choices=['hiremeai', 'HireMeAI'],
        help='Use predefined product'
    )
    product_group.add_argument(
        '--product-file',
        type=str,
        help='Path to custom product file'
    )

    # Platform settings
    platform_group = parser.add_mutually_exclusive_group()
    platform_group.add_argument(
        '--platform',
        type=str,
        choices=['twitter', 'x', 'linkedin', 'github', 'producthunt', 'hackernews', 'reddit', 'youtube', 'instagram', 'tiktok'],
        default='twitter',
        help='Single platform to use (default: twitter)'
    )
    platform_group.add_argument(
        '--platforms',
        type=str,
        help='Multiple platforms to rotate (comma-separated, e.g., twitter,github,reddit,youtube,tiktok)'
    )

    # Batch settings
    parser.add_argument(
        '--target-emails',
        type=int,
        default=50,
        help='Target emails to send per batch (default: 50)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=50,
        help='Leads to search per round (default: 50)'
    )
    parser.add_argument(
        '--rest-hours',
        type=int,
        default=5,
        help='Hours to rest between batches (default: 5)'
    )
    parser.add_argument(
        '--seeds-per-batch',
        type=int,
        default=5,
        help='Seed accounts per search round (default: 5)'
    )

    # Limits
    parser.add_argument(
        '--max-batches',
        type=int,
        default=None,
        help='Maximum number of batches (default: unlimited)'
    )

    # State
    parser.add_argument(
        '--reset',
        action='store_true',
        help='Reset campaign state and start fresh'
    )

    args = parser.parse_args()

    try:
        # Determine product file
        if args.product:
            product_file = str(SCRIPT_DIR / "products" / f"{args.product.lower()}.md")
            if not os.path.exists(product_file):
                logger.error(f"âŒ Product not found: {args.product}")
                sys.exit(1)
        else:
            product_file = args.product_file
            if not os.path.exists(product_file):
                logger.error(f"âŒ Product file not found: {product_file}")
                sys.exit(1)

        # Reset state if requested
        if args.reset:
            if os.path.exists("campaign_state.json"):
                os.remove("campaign_state.json")
                logger.info("ğŸ”„ Campaign state reset")

        # Determine platforms
        if args.platforms:
            # Multiple platforms (comma-separated)
            platforms = [p.strip().lower() for p in args.platforms.split(',')]
        else:
            # Single platform
            platforms = [args.platform.lower()]

        # Create and run campaign
        campaign = ContinuousCampaign(
            product_file=product_file,
            batch_size=args.batch_size,
            rest_hours=args.rest_hours,
            seeds_per_batch=args.seeds_per_batch,
            target_emails_per_batch=args.target_emails,
            platforms=platforms
        )

        campaign.run_continuous(max_batches=args.max_batches)

    except Exception as e:
        logger.error(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
