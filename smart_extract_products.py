#!/usr/bin/env python3
"""
æ™ºèƒ½æå– Product Hunt äº§å“ - ç›´æ¥ä»é¡µé¢å…ƒç´ æå–çœŸå®é“¾æ¥
"""
import sys
sys.path.insert(0, 'src')

from producthunt_commenter import ProductHuntCommenter
import os
import json
import time
from datetime import datetime

PRODUCT_LIST_FILE = "todays_producthunt_products.json"

def extract_products_from_page(commenter: ProductHuntCommenter) -> list:
    """ç›´æ¥ä»é¡µé¢å…ƒç´ æå–äº§å“ä¿¡æ¯"""
    print("\nğŸ” ä»é¡µé¢å…ƒç´ æå–äº§å“...")

    products = []

    try:
        # æ–¹æ³• 1: æŸ¥æ‰¾æ‰€æœ‰äº§å“æ ‡é¢˜å…ƒç´ å¹¶æå–é“¾æ¥
        # Product Hunt çš„äº§å“æ ‡é¢˜é€šå¸¸åŒ…å«äº§å“åç§°å’Œé“¾æ¥

        # å°è¯•å¤šç§æ–¹å¼æå–
        print("   å°è¯•æ–¹æ³• 1: æŸ¥æ‰¾äº§å“æ ‡é¢˜...")

        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(3)

        # ä½¿ç”¨ JavaScript æå–æ‰€æœ‰äº§å“ä¿¡æ¯
        js_code = """
        () => {
            const products = [];

            // å°è¯•å¤šç§é€‰æ‹©å™¨
            const selectors = [
                'div[data-test*="post"]',
                'article',
                'div[class*="item"]',
                'li[data-test*="post"]'
            ];

            for (const selector of selectors) {
                const elements = document.querySelectorAll(selector);

                for (const elem of elements) {
                    // æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
                    const titleLink = elem.querySelector('a[href*="/posts/"]');
                    if (!titleLink) continue;

                    const href = titleLink.getAttribute('href');
                    if (!href || href.includes('/posts/new') || href.includes('/posts/all')) continue;

                    // æå–äº§å“åç§°
                    const nameElem = elem.querySelector('h2, h3, strong, [class*="title"]') || titleLink;
                    const name = nameElem.textContent.trim();

                    // æå–æè¿°
                    const descElem = elem.querySelector('p, div[class*="tagline"], div[class*="description"]');
                    const desc = descElem ? descElem.textContent.trim() : '';

                    // æ„é€ å®Œæ•´ URL
                    const fullUrl = href.startsWith('/') ? 'https://www.producthunt.com' + href : href;

                    products.push({
                        name: name,
                        url: fullUrl.split('?')[0],  // ç§»é™¤æŸ¥è¯¢å‚æ•°
                        tagline: desc
                    });

                    if (products.length >= 10) break;
                }

                if (products.length > 0) break;
            }

            return products;
        }
        """

        extracted = commenter.page.evaluate(js_code)
        print(f"   JavaScript æå–åˆ° {len(extracted)} ä¸ªäº§å“")

        if extracted:
            for item in extracted:
                products.append({
                    'url': item['url'],
                    'name': item['name'][:50] if item['name'] else 'Product',  # é™åˆ¶é•¿åº¦
                    'tagline': item['tagline'][:200] if item['tagline'] else 'Product from Product Hunt',
                    'category': 'Various',
                    'description': item['tagline'][:200] if item['tagline'] else 'Product from Product Hunt'
                })

        # æ–¹æ³• 2: å¦‚æœæ–¹æ³• 1 å¤±è´¥ï¼Œå°è¯•ç‚¹å‡»å…ƒç´ æå–
        if not products:
            print("   å°è¯•æ–¹æ³• 2: éå†ç‚¹å‡»äº§å“...")

            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„äº§å“å…ƒç´ 
            product_elements = commenter.page.query_selector_all('a[href*="/posts/"]:not([href*="/posts/new"])')

            seen_urls = set()
            for elem in product_elements[:15]:
                try:
                    href = elem.get_attribute('href')
                    if not href or href in seen_urls:
                        continue

                    seen_urls.add(href)

                    # æ„é€ å®Œæ•´ URL
                    full_url = f"https://www.producthunt.com{href}" if href.startswith('/') else href
                    full_url = full_url.split('?')[0]

                    # æå–äº§å“å
                    text = elem.inner_text()
                    name = text.strip()[:50] if text else "Product"

                    products.append({
                        'url': full_url,
                        'name': name,
                        'tagline': f'{name} from Product Hunt',
                        'category': 'Various',
                        'description': f'Product from Product Hunt'
                    })

                    if len(products) >= 10:
                        break

                except Exception as e:
                    continue

        print(f"   âœ… æˆåŠŸæå– {len(products)} ä¸ªäº§å“")
        return products[:10]

    except Exception as e:
        print(f"   âŒ æå–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

def main():
    print("=" * 80)
    print("ğŸ” Product Hunt æ™ºèƒ½äº§å“æå–")
    print("=" * 80)

    commenter = ProductHuntCommenter()

    try:
        # è®¿é—®é¦–é¡µ
        print("\nğŸŒ è®¿é—® Product Hunt é¦–é¡µ...")
        commenter.setup_browser(headless=True)

        if not commenter.verify_login():
            print("âŒ ç™»å½•å¤±è´¥")
            return 1

        commenter.page.goto("https://www.producthunt.com", timeout=60000)
        print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
        time.sleep(10)

        # æ»šåŠ¨ä»¥åŠ è½½æ›´å¤š
        commenter.page.evaluate("window.scrollTo(0, 1500)")
        time.sleep(3)
        commenter.page.evaluate("window.scrollTo(0, 0)")
        time.sleep(2)

        # æˆªå›¾
        commenter.page.screenshot(path="ph_smart_extract.png")
        print("ğŸ“¸ æˆªå›¾ä¿å­˜: ph_smart_extract.png")

        # ä¿å­˜ HTML ç”¨äºè°ƒè¯•
        html = commenter.page.content()
        with open("ph_page_debug.html", 'w', encoding='utf-8') as f:
            f.write(html)
        print("ğŸ“„ HTML ä¿å­˜: ph_page_debug.html")

        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« /posts/ çš„é“¾æ¥
        all_links = commenter.page.query_selector_all('a')
        post_links = []
        for link in all_links:
            href = link.get_attribute('href')
            if href and '/posts/' in href:
                post_links.append(href)

        print(f"\nğŸ”— æ‰¾åˆ° {len(post_links)} ä¸ªåŒ…å« /posts/ çš„é“¾æ¥")
        unique_links = list(set(post_links))
        print(f"   å»é‡å: {len(unique_links)} ä¸ª")
        for i, link in enumerate(unique_links[:10], 1):
            print(f"   {i}. {link}")

        # æå–äº§å“
        products = extract_products_from_page(commenter)

        commenter.close_browser()

        if not products:
            print("\nâŒ æœªèƒ½æå–äº§å“")
            return 1

        # ä¿å­˜
        data = {
            "date": datetime.now().strftime('%Y-%m-%d'),
            "source": "Smart DOM Extraction",
            "products": products,
            "updated_at": datetime.now().isoformat()
        }

        with open(PRODUCT_LIST_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… æˆåŠŸä¿å­˜ {len(products)} ä¸ªäº§å“")
        print("\nğŸ“‹ äº§å“åˆ—è¡¨:")
        for i, p in enumerate(products, 1):
            print(f"   {i}. {p['name']}")
            print(f"      {p['url']}")

        print("\n" + "=" * 80)
        print("âœ… æ™ºèƒ½æå–å®Œæˆï¼")
        print("=" * 80)

        return 0

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        try:
            commenter.close_browser()
        except:
            pass

if __name__ == "__main__":
    sys.exit(main())
